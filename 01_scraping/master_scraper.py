#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ Another Eden ë§ˆìŠ¤í„° ìŠ¤í¬ë˜í¼
ìŠ¤í¬ë˜í•‘ â†’ ì´ë¯¸ì§€ ì •ë¦¬ â†’ CSV ìƒì„± â†’ ë°ì´í„° ì •ë¦¬ê¹Œì§€ ëª¨ë“  ì‘ì—…ì„ í†µí•© ì²˜ë¦¬
"""

import os
import sys
import time
import requests
import pandas as pd
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, parse_qs, urlparse
import re
import unicodedata
import mimetypes
import base64
from openpyxl import Workbook
from openpyxl.drawing.image import Image as OpenpyxlImage
import shutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
SCRAPING_DIR = PROJECT_ROOT / "01_scraping"
DATA_DIR = PROJECT_ROOT / "04_data"
CSV_DIR = DATA_DIR / "csv"
IMAGE_DIR = DATA_DIR / "images" / "character_art"

# ìŠ¤í¬ë˜í•‘ ì„¤ì •
BASE_URL = "https://anothereden.wiki"
TARGET_URL = "https://anothereden.wiki/w/Characters"
PERSONALITY_URL = "https://anothereden.wiki/w/Characters/Personality"


class MasterScraper:
    """í†µí•© ë§ˆìŠ¤í„° ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.resolve()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.character_data = []
        self.name_mapping = {}
        self.personality_mapping = {}
        self.setup_directories()
        
    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for directory in [CSV_DIR, IMAGE_DIR]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ í•˜ìœ„ í´ë”
        (IMAGE_DIR / "icons").mkdir(exist_ok=True)
        (IMAGE_DIR / "elements_equipment").mkdir(exist_ok=True)
        
        print(f"âœ… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ")
        print(f"   ğŸ“Š ë°ì´í„°: {CSV_DIR}")
        print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€: {IMAGE_DIR}")
    
    def load_name_mapping(self):
        """í•œê¸€ ì´ë¦„ ë§¤í•‘ ë¡œë“œ"""
        mapping_file = CSV_DIR / "Matching_names.csv"
        if mapping_file.exists():
            try:
                df = pd.read_csv(mapping_file, encoding='utf-8-sig')
                for _, row in df.iterrows():
                    if len(row) >= 2:
                        eng_name = str(row.iloc[0]).strip()
                        kor_name = str(row.iloc[1]).strip()
                        if eng_name and kor_name and kor_name != 'nan':
                            self.name_mapping[eng_name.lower()] = kor_name
                print(f"âœ… í•œê¸€ ë§¤ì¹­ ë¡œë“œ: {len(self.name_mapping)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ Matching_names.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def load_personality_mapping(self):
        """í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ"""
        mapping_file = CSV_DIR / "personality_matching.csv"
        if mapping_file.exists():
            try:
                df = pd.read_csv(mapping_file)
                for _, row in df.iterrows():
                    if 'English' in row and 'Korean' in row:
                        self.personality_mapping[row['English']] = row['Korean']
                print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ: {len(self.personality_mapping)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ personality_matching.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def convert_to_korean(self, english_name):
        """ì˜ì–´ ì´ë¦„ì„ í•œê¸€ë¡œ ë³€í™˜ (ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ê³ ë ¤)"""
        if not english_name:
            return english_name

        # ìŠ¤íƒ€ì¼ê³¼ ì¶•ì•½í˜• ë§¤í•‘
        style_map = {
            'Another Style': 'AS',
            'Extra Style': 'ES',
            'AS': 'AS',
            'ES': 'ES',
            'Alter': 'Alter',
            'Manifestation': 'M',
            'NS': 'NS'
        }

        base_name = english_name.strip()
        style_suffix = ""

        # ì •ê·œì‹ íŒ¨í„´ ìƒì„±
        pattern_str = r'\s*\(?(' + '|'.join(re.escape(k) for k in style_map.keys()) + r')\)?$'
        
        match = re.search(pattern_str, base_name, re.IGNORECASE)
        
        if match:
            matched_style_key = match.group(1)
            for key, abbreviation in style_map.items():
                if key.lower() == matched_style_key.lower():
                    style_suffix = " " + abbreviation
                    break
            
            base_name = base_name[:match.start()].strip()

        # ê¸°ë³¸ ì´ë¦„ì„ í•œê¸€ë¡œ ë³€í™˜
        korean_base = self.name_mapping.get(base_name.lower(), base_name)
        
        return korean_base + style_suffix
    
    def sanitize_filename(self, name):
        """íŒŒì¼ëª…ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        name = unicodedata.normalize('NFKC', name)
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        name = name.strip()
        return name
    
    def normalize_image_filename(self, korean_name, english_name):
        """ì´ë¯¸ì§€ íŒŒì¼ëª… ì •ê·œí™”"""
        # í•œê¸€ ì´ë¦„ì„ ìš°ì„  ì‚¬ìš©í•˜ë˜, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
        if korean_name and korean_name.strip():
            base_name = korean_name.strip()
        else:
            base_name = english_name.strip()
        
        # íŒŒì¼ëª… ì •ê·œí™”
        normalized = self.sanitize_filename(base_name)
        return f"{normalized}.png"
    
    def get_unique_filename(self, filepath):
        """ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±"""
        if not os.path.exists(filepath):
            return filepath
        base, ext = os.path.splitext(filepath)
        counter = 1
        while True:
            new_filepath = f"{base} ({counter}){ext}"
            if not os.path.exists(new_filepath):
                return new_filepath
            counter += 1
    
    def download_image(self, image_url, kor_name="", eng_name=""):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        if not image_url:
            return None

        full_image_url = urljoin(BASE_URL, image_url)
        try:
            # íŒŒì¼ ì´ë¦„ ë° í™•ì¥ì ê²°ì •
            parsed_url = urlparse(full_image_url)
            query_params = parse_qs(parsed_url.query)
            image_name_from_f = query_params.get('f', [None])[0]

            if image_name_from_f:
                image_name = os.path.basename(unquote(image_name_from_f))
            else:
                image_name = os.path.basename(unquote(parsed_url.path.split('?')[0]))

            if not image_name or image_name.lower() in ["thumb.php", "index.php"]:
                image_name = (eng_name or kor_name or "unknown") + ".png"

            base_name, ext = os.path.splitext(image_name)
            if not ext or len(ext) > 5:
                try:
                    head_resp = requests.head(full_image_url, timeout=5, allow_redirects=True)
                    head_resp.raise_for_status()
                    content_type = head_resp.headers.get('Content-Type')
                    if content_type:
                        guessed_ext = mimetypes.guess_extension(content_type.split(';')[0])
                        ext = guessed_ext if guessed_ext and guessed_ext != '.jpe' else ('.jpg' if guessed_ext == '.jpe' else '.png')
                    else:
                        ext = ".png"
                except requests.exceptions.RequestException:
                    ext = ".png"

            # ìµœì¢… íŒŒì¼ ì´ë¦„ ìƒì„± (ì •ê·œí™”ëœ í•œê¸€ëª… ìš°ì„ )
            final_name = self.normalize_image_filename(kor_name, eng_name)
            final_filename = final_name

            # ì €ì¥ ê²½ë¡œ ì„¤ì • ë° ì¤‘ë³µ í™•ì¸
            save_path = IMAGE_DIR / final_filename
            save_path.parent.mkdir(parents=True, exist_ok=True)

            if save_path.exists() and save_path.stat().st_size > 0:
                return str(save_path.relative_to(self.project_root).as_posix())

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            img_response = requests.get(full_image_url, headers=self.headers, timeout=20, stream=True)
            img_response.raise_for_status()

            with open(save_path, 'wb') as f:
                for chunk in img_response.iter_content(8192):
                    f.write(chunk)
            
            return str(save_path.relative_to(self.project_root).as_posix())

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {full_image_url} ({e})")
            return None
        except Exception as e:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {full_image_url} ({e})")
            return None
    
    def download_icon(self, icon_url, alt_text, subfolder):
        """ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
        if not icon_url:
            return ""
        
        try:
            # URL ì •ê·œí™”
            if not icon_url.startswith('http'):
                icon_url = urljoin(BASE_URL, icon_url)
            
            # íŒŒì¼ëª… ìƒì„±
            parsed_url = urlparse(icon_url)
            query_params = parse_qs(parsed_url.query)
            icon_name_from_f = query_params.get('f', [None])[0]
            
            if icon_name_from_f:
                icon_name = os.path.basename(unquote(icon_name_from_f))
            else:
                icon_name = os.path.basename(unquote(parsed_url.path.split('?')[0]))
            
            # í™•ì¥ì ì²˜ë¦¬
            if not icon_name or icon_name.lower() in ["thumb.php", "index.php"]:
                try:
                    head_resp = requests.head(icon_url, timeout=3, allow_redirects=True)
                    head_resp.raise_for_status()
                    content_type = head_resp.headers.get('Content-Type')
                    if content_type:
                        guessed_ext = mimetypes.guess_extension(content_type.split(';')[0])
                        if guessed_ext:
                            ext = guessed_ext
                        else:
                            ext = ".png"
                    else:
                        ext = ".png"
                except requests.exceptions.RequestException:
                    ext = ".png"
            else:
                base_name, ext = os.path.splitext(icon_name)
                if not ext or len(ext) > 5:
                    ext = ".png"
            
            # ì•„ì´ì½˜ íŒŒì¼ëª… ìƒì„±
            icon_filename = f"{alt_text.replace(' ', '_')}{ext}"
            icon_filename = self.sanitize_filename(icon_filename)
            
            # ì €ì¥ ê²½ë¡œ ì„¤ì •
            icon_dir = IMAGE_DIR / subfolder
            icon_dir.mkdir(exist_ok=True)
            save_path = icon_dir / icon_filename
            save_path = self.get_unique_filename(save_path)
            
            # ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ
            response = requests.get(icon_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            print(f"  ğŸ¯ ì•„ì´ì½˜ ì €ì¥: {save_path.name}")
            return str(save_path.relative_to(self.project_root).as_posix())
            
        except Exception as e:
            print(f"âš ï¸ ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({icon_url}): {e}")
            return ""
    
    def scrape_character_list(self):
        """ìºë¦­í„° ëª©ë¡ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        print("ğŸ“¡ ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        try:
            response = requests.get(TARGET_URL, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table', class_='wikitable')
            if not tables:
                print("âŒ ìºë¦­í„° í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            characters = []
            for table in tables:
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows:
                    cols = row.find_all(['td', 'th'])
                    if len(cols) < 2:
                        continue
                    
                    # ìºë¦­í„° ë§í¬ ë° ì´ë¦„ ì¶”ì¶œ
                    char_link = cols[0].find('a')
                    if not char_link:
                        continue
                    
                    eng_name = char_link.get('title', '').strip()
                    detail_url = urljoin(BASE_URL, char_link.get('href', ''))
                    
                    if not eng_name or not detail_url:
                        continue
                    
                    # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                    img_tag = cols[0].find('img')
                    img_url = img_tag.get('src', '') if img_tag else ''
                    
                    characters.append({
                        'english_name': eng_name,
                        'detail_url': detail_url,
                        'image_url': img_url
                    })
            
            print(f"âœ… ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(characters)}ê°œ")
            return characters
            
        except Exception as e:
            print(f"âŒ ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def clean_scraped_data(self, data):
        """ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„° ì •ë¦¬"""
        cleaned_data = {}
        
        # í¬ê·€ë„ ì •ë¦¬ (SA ì •ë³´ í¬í•¨)
        if 'rarity' in data:
            rarity = data['rarity']
            # SA ì •ë³´ í™•ì¸
            is_sa = 'SA' in rarity.upper() or 'Stellar Awakened' in rarity or 'ì„±ë„ê°ì„±' in rarity
            
            # ìˆ«ìì™€ ë³„í‘œ ì¶”ì¶œ
            rarity_match = re.search(r'(\d+)\s*â˜…', rarity)
            if rarity_match:
                star_count = rarity_match.group(1)
                if is_sa:
                    cleaned_data['rarity'] = f"{star_count}â˜… ì„±ë„ê°ì„±"
                else:
                    cleaned_data['rarity'] = f"{star_count}â˜…"
            else:
                if is_sa:
                    cleaned_data['rarity'] = "ì„±ë„ê°ì„±"
                else:
                    cleaned_data['rarity'] = rarity
        
        # ì†ì„± ì •ë¦¬
        if 'elements' in data:
            elements = data['elements']
            # ê¸°ë³¸ ì†ì„±ë§Œ ì¶”ì¶œ (Fire, Water, Earth, Wind, Light, Dark)
            basic_elements = []
            element_keywords = ['Fire', 'Water', 'Earth', 'Wind', 'Light', 'Dark', 'Crystal']
            for element in element_keywords:
                if element.lower() in elements.lower():
                    basic_elements.append(element)
            
            if basic_elements:
                cleaned_data['elements'] = ', '.join(basic_elements)
            else:
                cleaned_data['elements'] = elements
        
        # ë¬´ê¸° ì •ë¦¬
        if 'weapons' in data:
            weapons = data['weapons']
            # ê¸°ë³¸ ë¬´ê¸°ë§Œ ì¶”ì¶œ
            basic_weapons = []
            weapon_keywords = ['Sword', 'Katana', 'Axe', 'Hammer', 'Spear', 'Bow', 'Staff', 'Fist']
            for weapon in weapon_keywords:
                if weapon.lower() in weapons.lower():
                    basic_weapons.append(weapon)
            
            if basic_weapons:
                cleaned_data['weapons'] = ', '.join(basic_weapons)
            else:
                cleaned_data['weapons'] = weapons
        
        return cleaned_data

    def scrape_character_details(self, detail_url, eng_name):
        """ìºë¦­í„° ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (ì•„ì´ì½˜ í¬í•¨)"""
        try:
            response = requests.get(detail_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            data = {}
            element_icons = []
            weapon_icons = []
            
            # ë‹¤ì–‘í•œ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì°¾ê¸°
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows:
                    cells = row.find_all(['th', 'td'])
                    if len(cells) < 2:
                        continue
                    
                    # í—¤ë”ì™€ ê°’ ë¶„ë¦¬
                    header_cell = cells[0]
                    value_cell = cells[-1] if len(cells) > 1 else None
                    
                    if not header_cell or not value_cell:
                        continue
                    
                    header_text = header_cell.get_text(strip=True).lower()
                    value_text = value_cell.get_text(strip=True)
                    
                    # í¬ê·€ë„ ì°¾ê¸° (SA ì •ë³´ í¬í•¨)
                    if any(keyword in header_text for keyword in ['rarity', 'í¬ê·€ë„', 'star', 'ë³„']):
                        if 'â˜…' in value_text or 'star' in value_text.lower() or 'SA' in value_text.upper() or 'Stellar Awakened' in value_text:
                            data['rarity'] = value_text
                    
                    # ì†ì„± ì°¾ê¸°
                    elif any(keyword in header_text for keyword in ['element', 'ì†ì„±', 'type']):
                        if any(element in value_text.lower() for element in ['fire', 'water', 'earth', 'wind', 'light', 'dark', 'crystal']):
                            data['elements'] = value_text
                            
                            # ì†ì„± ì•„ì´ì½˜ ì°¾ê¸°
                            img_tags = value_cell.find_all('img')
                            for img in img_tags:
                                src = img.get('src', '')
                                alt = img.get('alt', '').lower()
                                if src and any(element in alt for element in ['fire', 'water', 'earth', 'wind', 'light', 'dark', 'crystal']):
                                    icon_path = self.download_icon(src, alt, "elements_equipment")
                                    if icon_path:
                                        element_icons.append(icon_path)
                    
                    # ë¬´ê¸° ì°¾ê¸°
                    elif any(keyword in header_text for keyword in ['weapon', 'ë¬´ê¸°', 'arms']):
                        if any(weapon in value_text.lower() for weapon in ['sword', 'katana', 'axe', 'hammer', 'spear', 'bow', 'staff', 'fist']):
                            data['weapons'] = value_text
                            
                            # ë¬´ê¸° ì•„ì´ì½˜ ì°¾ê¸°
                            img_tags = value_cell.find_all('img')
                            for img in img_tags:
                                src = img.get('src', '')
                                alt = img.get('alt', '').lower()
                                if src and any(weapon in alt for weapon in ['sword', 'katana', 'axe', 'hammer', 'spear', 'bow', 'staff', 'fist']):
                                    icon_path = self.download_icon(src, alt, "elements_equipment")
                                    if icon_path:
                                        weapon_icons.append(icon_path)
            
            # ê³ í™”ì§ˆ ì´ë¯¸ì§€ ì¶”ì¶œ
            img_tag = soup.find('img', class_='thumbimage') or soup.find('img', class_='infobox-image')
            if img_tag:
                img_src = img_tag.get('src', '')
                if img_src.startswith('http'):
                    data['high_res_image_url'] = img_src
                else:
                    data['high_res_image_url'] = urljoin(BASE_URL, img_src)
            
            # ë°ì´í„° ì •ë¦¬
            cleaned_data = self.clean_scraped_data(data)
            
            # ì•„ì´ì½˜ ì •ë³´ ì¶”ê°€
            cleaned_data['element_icons'] = element_icons
            cleaned_data['weapon_icons'] = weapon_icons
            
            # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
            if cleaned_data:
                print(f"  ğŸ“Š {eng_name}: í¬ê·€ë„={cleaned_data.get('rarity', 'N/A')}, ì†ì„±={cleaned_data.get('elements', 'N/A')}, ë¬´ê¸°={cleaned_data.get('weapons', 'N/A')}")
                if element_icons:
                    print(f"    ğŸ¯ ì†ì„± ì•„ì´ì½˜: {len(element_icons)}ê°œ")
                if weapon_icons:
                    print(f"    âš”ï¸ ë¬´ê¸° ì•„ì´ì½˜: {len(weapon_icons)}ê°œ")
            
            return cleaned_data
            
        except Exception as e:
            print(f"âš ï¸ {eng_name} ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def scrape_all_personalities(self):
        """ì „ì²´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘"""
        print("ğŸ­ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì¤‘...")
        try:
            response = requests.get(PERSONALITY_URL, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # wikitable í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ ì°¾ê¸°
            tables = soup.find_all('table', class_='wikitable')
            character_personalities = {}
            
            for table in tables:
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        # í¼ìŠ¤ë„ë¦¬í‹° ì´ë¦„
                        personality_cell = cells[0]
                        personality_eng = personality_cell.get_text(strip=True)
                        personality_kor = self.personality_mapping.get(personality_eng, personality_eng)
                        
                        # ìºë¦­í„° ëª©ë¡
                        characters_cell = cells[1]
                        character_links = characters_cell.find_all('a')
                        characters_found = []
                        
                        for link in character_links:
                            href = link.get('href', '')
                            if (href.startswith('/w/') and 
                                'Character' not in href and 
                                'Personality' not in href and
                                'Special:' not in href and
                                'Category:' not in href):
                                
                                char_name = link.get_text(strip=True)
                                if char_name and len(char_name) > 1:
                                    characters_found.append(char_name)
                        
                        # ê° ìºë¦­í„°ì— í¼ìŠ¤ë„ë¦¬í‹° ì¶”ê°€
                        for char_name in characters_found:
                            if char_name not in character_personalities:
                                character_personalities[char_name] = []
                            character_personalities[char_name].append(personality_kor)
            
            print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(character_personalities)}ëª…")
            return character_personalities
            
        except Exception as e:
            print(f"âŒ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def create_excel_with_images(self, characters):
        """ì´ë¯¸ì§€ í¬í•¨ ì—‘ì…€ íŒŒì¼ ìƒì„±"""
        print("ğŸ“Š ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘...")
        try:
            wb = Workbook()
            ws = wb.active
            ws.title = "Characters"
            
            headers = ['English Name', 'Korean Name', 'Rarity', 'Elements', 'Weapons', 'Personalities', 'Image Path']
            ws.append(headers)
            
            for char in characters:
                row = [
                    char.get('english_name', ''),
                    char.get('korean_name', ''),
                    char.get('rarity', ''),
                    char.get('elements', ''),
                    char.get('weapons', ''),
                    char.get('personalities', ''),
                    char.get('image_path', '')
                ]
                ws.append(row)
            
            for column in ws.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws.column_dimensions[column_letter].width = adjusted_width
            
            excel_path = CSV_DIR / "another_eden_characters_detailed.xlsx"
            wb.save(excel_path)
            print(f"âœ… ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {excel_path}")
            return excel_path
        except Exception as e:
            print(f"âŒ ì—‘ì…€ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def generate_csv_files(self, characters, personality_data):
        """í†µì¼ëœ CSV íŒŒì¼ë“¤ ìƒì„± (í€´ì¦ˆìš© + ë£°ë ›ìš©)"""
        print("ğŸ“‹ í†µì¼ëœ CSV íŒŒì¼ë“¤ ìƒì„± ì¤‘...")
        
        # 1. í€´ì¦ˆìš© ë°ì´í„° (í†µì¼ëœ êµ¬ì¡°)
        quiz_data = []
        for char in characters:
            # í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            base_eng_name = re.sub(r'\s*\(.*\)$', '', char.get('english_name', '')).strip()
            personalities = personality_data.get(base_eng_name, [])
            
            # í¼ìŠ¤ë„ë¦¬í‹° í•œê¸€ ë³€í™˜
            korean_personalities = []
            for personality in personalities:
                korean_personality = self.personality_mapping.get(personality, personality)
                korean_personalities.append(korean_personality)
            
            quiz_data.append({
                'ìºë¦­í„°ëª…': char.get('korean_name', ''),
                'English_Name': char.get('english_name', ''),
                'ìºë¦­í„°ì•„ì´ì½˜ê²½ë¡œ': char.get('image_path', ''),
                'í¬ê·€ë„': char.get('rarity', ''),
                'ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸': char.get('elements', ''),
                'ë¬´ê¸°ëª…ë¦¬ìŠ¤íŠ¸': char.get('weapons', ''),
                'í¼ìŠ¤ë„ë¦¬í‹°ë¦¬ìŠ¤íŠ¸': ', '.join(korean_personalities)
            })
        
        quiz_df = pd.DataFrame(quiz_data)
        quiz_csv_path = CSV_DIR / "eden_quiz_data.csv"
        quiz_df.to_csv(quiz_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í€´ì¦ˆ ë°ì´í„° ì €ì¥: {quiz_csv_path}")

        # 2. ë£°ë ›ìš© ë°ì´í„° (ì•„ì´ì½˜ í¬í•¨ í™•ì¥ êµ¬ì¡°)
        roulette_data = []
        for char in characters:
            # í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            base_eng_name = re.sub(r'\s*\(.*\)$', '', char.get('english_name', '')).strip()
            personalities = personality_data.get(base_eng_name, [])
            
            # í¼ìŠ¤ë„ë¦¬í‹° í•œê¸€ ë³€í™˜
            korean_personalities = []
            for personality in personalities:
                korean_personality = self.personality_mapping.get(personality, personality)
                korean_personalities.append(korean_personality)
            
            # ì•„ì´ì½˜ ê²½ë¡œ ìƒì„± (ì‹¤ì œ ìŠ¤í¬ë˜í•‘ëœ ì•„ì´ì½˜ ì‚¬ìš©)
            element_icons = char.get('element_icons', [])
            weapon_icons = char.get('weapon_icons', [])
            armor_icons = []
            
            # ìŠ¤í¬ë˜í•‘ëœ ì•„ì´ì½˜ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ìƒì„±
            if not element_icons and char.get('elements'):
                elements = char.get('elements').split(',')
                for element in elements:
                    element = element.strip()
                    if element:
                        icon_path = f"04_data/images/character_art/elements_equipment/{element.lower()}.png"
                        element_icons.append(icon_path)
            
            if not weapon_icons and char.get('weapons'):
                weapons = char.get('weapons').split(',')
                for weapon in weapons:
                    weapon = weapon.strip()
                    if weapon:
                        icon_path = f"04_data/images/character_art/elements_equipment/{weapon.lower()}.png"
                        weapon_icons.append(icon_path)
            
            roulette_data.append({
                'ìºë¦­í„°ëª…': char.get('korean_name', ''),
                'English_Name': char.get('english_name', ''),
                'ìºë¦­í„°ì•„ì´ì½˜ê²½ë¡œ': char.get('image_path', ''),
                'í¬ê·€ë„': char.get('rarity', ''),
                'ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸': char.get('elements', ''),
                'ì†ì„±_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸': '|'.join(element_icons),
                'ë¬´ê¸°ëª…ë¦¬ìŠ¤íŠ¸': char.get('weapons', ''),
                'ë¬´ê¸°_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸': '|'.join(weapon_icons),
                'ë°©ì–´êµ¬ëª…ë¦¬ìŠ¤íŠ¸': '',  # í˜„ì¬ ìŠ¤í¬ë˜í¼ì—ì„œ ë°©ì–´êµ¬ ì •ë³´ ì—†ìŒ
                'ë°©ì–´êµ¬_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸': '|'.join(armor_icons),
                'í¼ìŠ¤ë„ë¦¬í‹°ë¦¬ìŠ¤íŠ¸': ', '.join(korean_personalities)
            })
        
        roulette_df = pd.DataFrame(roulette_data)
        roulette_csv_path = CSV_DIR / "eden_roulette_data.csv"
        roulette_df.to_csv(roulette_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ë£°ë › ë°ì´í„° ì €ì¥: {roulette_csv_path}")
        
        # 3. í¼ìŠ¤ë„ë¦¬í‹° ì „ìš© CSV (ë£°ë › ì•±ìš©)
        personality_data_list = []
        for char in characters:
            base_eng_name = re.sub(r'\s*\(.*\)$', '', char.get('english_name', '')).strip()
            personalities = personality_data.get(base_eng_name, [])
            
            korean_personalities = []
            for personality in personalities:
                korean_personality = self.personality_mapping.get(personality, personality)
                korean_personalities.append(korean_personality)
            
            personality_data_list.append({
                'Korean_Name': char.get('korean_name', ''),
                'English_Name': char.get('english_name', ''),
                'Personalities_List': ', '.join(korean_personalities)
            })
        
        personality_df = pd.DataFrame(personality_data_list)
        personality_csv_path = CSV_DIR / "character_personalities.csv"
        personality_df.to_csv(personality_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ì €ì¥: {personality_csv_path}")

        # 3. í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° (character_personalities.csv)
        personality_list = []
        for char_name, personalities in personality_data.items():
            personality_list.append({
                'Character': self.convert_to_korean(char_name),
                'Personalities_Count': len(personalities),
                'Personalities_List': '|'.join(personalities)
            })
        
        personality_df = pd.DataFrame(personality_list)
        personality_df.sort_values('Personalities_Count', ascending=False, inplace=True)
        personality_csv_path = CSV_DIR / "character_personalities.csv"
        personality_df.to_csv(personality_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ì €ì¥: {personality_csv_path}")
        
        return quiz_csv_path, roulette_csv_path, personality_csv_path

    def run_full_scraping(self):
        """ì „ì²´ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print("ğŸš€ Another Eden í†µí•© ìŠ¤í¬ë˜í¼ ì‹œì‘")
        print("=" * 60)
        
        self.load_name_mapping()
        self.load_personality_mapping()
        
        characters = self.scrape_character_list()
        if not characters:
            print("âŒ ìºë¦­í„° ë°ì´í„° ì—†ìŒ")
            return False
        
        # ì „ì²´ ìºë¦­í„° ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
        print("ğŸš€ ì „ì²´ ìºë¦­í„° ì²˜ë¦¬ ëª¨ë“œ: 372ê°œ ìºë¦­í„°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        # characters = characters[:10]  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ë¹„í™œì„±í™”
        
        personality_data = self.scrape_all_personalities()
        
        # --- Phase 1: Scrape all data ---
        print("\n--- Phase 1: ëª¨ë“  ë°ì´í„° ìŠ¤í¬ë˜í•‘ ---")
        all_details = []
        print("ğŸ“„ ìºë¦­í„° ìƒì„¸ ì •ë³´ ì¼ê´„ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        for i, char in enumerate(characters, 1):
            eng_name = char['english_name']
            print(f"[{i}/{len(characters)}] {eng_name} ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            details = self.scrape_character_details(char['detail_url'], eng_name)
            all_details.append(details)
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸° ì‹œê°„ ì¦ê°€

        # --- Phase 2: Process all data ---
        print("\n--- Phase 2: ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ---")
        processed_characters = []
        for i, char_data in enumerate(characters):
            eng_name = char_data['english_name']
            kor_name = self.convert_to_korean(eng_name)
            details = all_details[i]

            # Get personalities - ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ì œê±° í›„ ë§¤ì¹­
            base_eng_name = re.sub(r'\s*\(.*\)$', '', eng_name).strip()
            personalities = personality_data.get(base_eng_name, [])
            
            # í¼ìŠ¤ë„ë¦¬í‹° í•œê¸€ ë³€í™˜
            korean_personalities = []
            for personality in personalities:
                korean_personality = self.personality_mapping.get(personality, personality)
                korean_personalities.append(korean_personality)

            processed_char = {
                **char_data,
                'korean_name': kor_name,
                'rarity': details.get('rarity', ''),
                'elements': details.get('elements', ''),
                'weapons': details.get('weapons', ''),
                'personalities': ', '.join(korean_personalities),
                'high_res_image_url': details.get('high_res_image_url', ''),
                'image_path': ''  # Initialize path
            }
            processed_characters.append(processed_char)

        # --- Phase 3: Download images ---
        print("\n--- Phase 3: ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ---")
        for i, char in enumerate(processed_characters, 1):
            kor_name = char['korean_name']
            eng_name = char['english_name']
            image_url = char.get('high_res_image_url') or char.get('image_url')
            
            print(f"[{i}/{len(processed_characters)}] {kor_name} ({eng_name}) ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            if image_url:
                image_path = self.download_image(image_url, kor_name=kor_name, eng_name=eng_name)
                char['image_path'] = image_path or ''
            else:
                print(f"  âŒ ì´ë¯¸ì§€ URL ì—†ìŒ")
        
        # Replace original characters list with the fully processed one
        characters = processed_characters
        
        excel_path = self.create_excel_with_images(characters)
        csv_paths = self.generate_csv_files(characters, personality_data)
        
        print("\nğŸ‰ í†µí•© ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì´ ìºë¦­í„° ìˆ˜: {len(characters)}")
        if excel_path:
            print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼: {excel_path}")
        if csv_paths:
            print(f"ğŸ’¾ CSV íŒŒì¼ë“¤: {', '.join(map(str, csv_paths))}")
        
        return True


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = MasterScraper()
    success = scraper.run_full_scraping()
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
