#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ Another Eden ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í¼
ìºë¦­í„°ë“¤ì˜ ì†ì„± ì •ë³´ë¥¼ ì™„ì „íˆ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë°ì´í„°ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import requests
import pandas as pd
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, parse_qs, urlparse
import re
import unicodedata
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
DATA_DIR = PROJECT_ROOT / "04_data"
CSV_DIR = DATA_DIR / "csv"
IMAGE_DIR = DATA_DIR / "images" / "character_art"

# ìŠ¤í¬ë˜í•‘ ì„¤ì •
BASE_URL = "https://anothereden.wiki"
TARGET_URL = "https://anothereden.wiki/w/Characters"

class ElementScraper:
    """ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.element_mapping = {
            'Fire': 'ë¶ˆ',
            'Water': 'ë¬¼', 
            'Earth': 'ë•…',
            'Wind': 'ë°”ëŒ',
            'Thunder': 'ë²ˆê°œ',
            'Shade': 'ê·¸ë¦¼ì',
            'Crystal': 'ê²°ì •',
            'Light': 'ë¹›',
            'Dark': 'ì–´ë‘ '
        }
        self.setup_directories()
        
    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        CSV_DIR.mkdir(parents=True, exist_ok=True)
        print(f"âœ… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ: {CSV_DIR}")
    
    def load_existing_data(self):
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        csv_path = CSV_DIR / "eden_unified_data.csv"
        if csv_path.exists():
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ ìºë¦­í„°")
            return df
        else:
            print("âš ï¸ ê¸°ì¡´ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def scrape_character_elements(self, character_url, character_name):
        """ìºë¦­í„°ì˜ ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        try:
            response = requests.get(character_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # infoboxì—ì„œ ì†ì„± ì •ë³´ ì¶”ì¶œ
            infobox = soup.find('table', class_='infobox')
            if not infobox:
                return None
            
            elements = []
            rows = infobox.find_all('tr')
            
            for row in rows:
                header = row.find(['th', 'td'])
                if not header:
                    continue
                
                header_text = header.get_text(strip=True)
                if any(keyword in header_text for keyword in ['Element', 'ì†ì„±', 'Type']):
                    element_cell = row.find_all(['td', 'th'])[-1]
                    element_text = element_cell.get_text(strip=True)
                    
                    # ì˜ì–´ ì†ì„±ì„ í•œê¸€ë¡œ ë³€í™˜
                    for eng, kor in self.element_mapping.items():
                        if eng.lower() in element_text.lower():
                            elements.append(kor)
                    
                    # í•œê¸€ ì†ì„± ì§ì ‘ í™•ì¸
                    for kor in self.element_mapping.values():
                        if kor in element_text:
                            elements.append(kor)
            
            # ì¤‘ë³µ ì œê±°
            elements = list(set(elements))
            return ', '.join(elements) if elements else None
            
        except Exception as e:
            print(f"âš ï¸ {character_name} ì†ì„± ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def scrape_all_character_elements(self):
        """ëª¨ë“  ìºë¦­í„°ì˜ ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        print("ğŸ”¥ ëª¨ë“  ìºë¦­í„° ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        
        try:
            response = requests.get(TARGET_URL, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ìºë¦­í„° ë§í¬ë“¤ ì°¾ê¸°
            character_links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                if (href.startswith('/w/') and 
                    'Character' not in href and 
                    'Personality' not in href and
                    'Special:' not in href and
                    'Category:' not in href and
                    'Template:' not in href):
                    
                    character_name = link.get_text(strip=True)
                    if character_name and len(character_name) > 1:
                        character_links.append({
                            'name': character_name,
                            'url': urljoin(BASE_URL, href)
                        })
            
            print(f"ğŸ“‹ ë°œê²¬ëœ ìºë¦­í„°: {len(character_links)}ê°œ")
            
            # ê° ìºë¦­í„°ì˜ ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘
            character_elements = {}
            for i, char in enumerate(character_links, 1):
                print(f"ğŸ” [{i}/{len(character_links)}] {char['name']} ì†ì„± ìŠ¤í¬ë˜í•‘ ì¤‘...")
                
                elements = self.scrape_character_elements(char['url'], char['name'])
                if elements:
                    character_elements[char['name']] = elements
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1)
            
            print(f"âœ… ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(character_elements)}ê°œ")
            return character_elements
            
        except Exception as e:
            print(f"âŒ ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def update_existing_data(self, element_data):
        """ê¸°ì¡´ ë°ì´í„°ì— ì†ì„± ì •ë³´ ì—…ë°ì´íŠ¸"""
        df = self.load_existing_data()
        if df.empty:
            print("âš ï¸ ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì˜ì–´ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­
        updated_count = 0
        for index, row in df.iterrows():
            eng_name = row.get('English_Name', '')
            if eng_name and eng_name in element_data:
                df.at[index, 'ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸'] = element_data[eng_name]
                updated_count += 1
        
        # ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ì €ì¥
        output_path = CSV_DIR / "eden_unified_data_updated.csv"
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê°œ ìºë¦­í„° ì†ì„± ì •ë³´ ì—…ë°ì´íŠ¸")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {output_path}")
        
        # ì†ì„± ì •ë³´ í†µê³„
        element_stats = {}
        for elements in df['ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸'].dropna():
            for element in elements.split(', '):
                element = element.strip()
                if element:
                    element_stats[element] = element_stats.get(element, 0) + 1
        
        print("\nğŸ“Š ì†ì„± ì •ë³´ í†µê³„:")
        for element, count in sorted(element_stats.items()):
            print(f"   {element}: {count}ê°œ ìºë¦­í„°")
    
    def run_full_scraping(self):
        """ì „ì²´ ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print("ğŸš€ Another Eden ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        print("=" * 50)
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        df = self.load_existing_data()
        if df.empty:
            print("âŒ ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return
        
        # ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘
        element_data = self.scrape_all_character_elements()
        
        if element_data:
            # ë°ì´í„° ì—…ë°ì´íŠ¸
            self.update_existing_data(element_data)
        else:
            print("âŒ ì†ì„± ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print("=" * 50)
        print("âœ… ì†ì„± ì •ë³´ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = ElementScraper()
    scraper.run_full_scraping()

if __name__ == "__main__":
    main() 