#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ Another Eden ë§ˆìŠ¤í„° ìŠ¤í¬ë˜í¼
ìŠ¤í¬ë˜í•‘ â†’ ì´ë¯¸ì§€ ì •ë¦¬ â†’ CSV ìƒì„± â†’ ë°ì´í„° ì •ë¦¬ê¹Œì§€ ëª¨ë“  ì‘ì—…ì„ í†µí•© ì²˜ë¦¬
"""

import os
import sys
import time
import requests
import pandas as pd
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, parse_qs, urlparse
import re
import unicodedata
import mimetypes
import base64
from openpyxl import Workbook
from openpyxl.drawing.image import Image as OpenpyxlImage
import shutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
SCRAPING_DIR = PROJECT_ROOT / "01_scraping"
DATA_DIR = PROJECT_ROOT / "04_data"
CSV_DIR = DATA_DIR / "csv"
IMAGE_DIR = DATA_DIR / "images" / "character_art"

# ìŠ¤í¬ë˜í•‘ ì„¤ì •
BASE_URL = "https://anothereden.wiki"
TARGET_URL = "https://anothereden.wiki/w/Characters"
PERSONALITY_URL = "https://anothereden.wiki/w/Characters/Personality"


class MasterScraper:
    """í†µí•© ë§ˆìŠ¤í„° ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.character_data = []
        self.name_mapping = {}
        self.personality_mapping = {}
        self.setup_directories()
        
    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for directory in [CSV_DIR, IMAGE_DIR]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ í•˜ìœ„ í´ë”
        (IMAGE_DIR / "icons").mkdir(exist_ok=True)
        (IMAGE_DIR / "elements_equipment").mkdir(exist_ok=True)
        
        print(f"âœ… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ")
        print(f"   ğŸ“Š ë°ì´í„°: {CSV_DIR}")
        print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€: {IMAGE_DIR}")
    
    def load_name_mapping(self):
        """í•œê¸€ ì´ë¦„ ë§¤í•‘ ë¡œë“œ"""
        mapping_file = CSV_DIR / "Matching_names.csv"
        if mapping_file.exists():
            try:
                df = pd.read_csv(mapping_file, encoding='utf-8-sig')
                for _, row in df.iterrows():
                    if len(row) >= 2:
                        eng_name = str(row.iloc[0]).strip()
                        kor_name = str(row.iloc[1]).strip()
                        if eng_name and kor_name and kor_name != 'nan':
                            self.name_mapping[eng_name.lower()] = kor_name
                print(f"âœ… í•œê¸€ ë§¤ì¹­ ë¡œë“œ: {len(self.name_mapping)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ Matching_names.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def load_personality_mapping(self):
        """í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ"""
        mapping_file = SCRAPING_DIR / "personality_matching.csv"
        if mapping_file.exists():
            try:
                df = pd.read_csv(mapping_file)
                for _, row in df.iterrows():
                    if 'English' in row and 'Korean' in row:
                        self.personality_mapping[row['English']] = row['Korean']
                print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ: {len(self.personality_mapping)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ personality_matching.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def convert_to_korean(self, english_name):
        """ì˜ì–´ ì´ë¦„ì„ í•œê¸€ë¡œ ë³€í™˜ (ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ê³ ë ¤)"""
        if not english_name:
            return english_name
        
        # ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ë¶„ë¦¬
        style_patterns = [
            r'\s+(AS)$', r'\s+(ES)$', r'\s+(NS)$',
            r'\s+(Another\s*Style)$', r'\s+(Extra\s*Style)$',
            r'\s+(Manifestation)$', r'\s+(Alter)$',
        ]
        
        base_name = english_name
        style_suffix = ""
        
        for pattern in style_patterns:
            match = re.search(pattern, english_name, re.IGNORECASE)
            if match:
                style_suffix = " " + match.group(1)
                base_name = english_name[:match.start()]
                break
        
        # í•œê¸€ ë§¤ì¹­
        korean_base = self.name_mapping.get(base_name.lower(), base_name)
        return korean_base + style_suffix
    
    def sanitize_filename(self, name):
        """íŒŒì¼ëª…ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        name = unicodedata.normalize('NFKC', name)
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        name = name.strip()
        return name
    
    def get_unique_filename(self, filepath):
        """ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±"""
        if not os.path.exists(filepath):
            return filepath
        base, ext = os.path.splitext(filepath)
        counter = 1
        while True:
            new_filepath = f"{base} ({counter}){ext}"
            if not os.path.exists(new_filepath):
                return new_filepath
            counter += 1
    
    def download_image(self, image_url, subfolder=""):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
        if not image_url:
            return None
        
        full_image_url = urljoin(BASE_URL, image_url)
        try:
            parsed_url = urlparse(full_image_url)
            query_params = parse_qs(parsed_url.query)
            image_name_from_f = query_params.get('f', [None])[0]
            
            if image_name_from_f:
                image_name = os.path.basename(unquote(image_name_from_f))
            else:
                image_name = os.path.basename(unquote(parsed_url.path.split('?')[0]))

            if not image_name or image_name.lower() in ["thumb.php", "index.php"]:
                temp_name = unquote(full_image_url.split('/')[-1].split('?')[0])
                image_name = (temp_name[:50] + ".png") if temp_name else "unknown_image.png"

            base_name, ext = os.path.splitext(image_name)
            if not ext or len(ext) > 5:
                try:
                    head_resp = requests.head(full_image_url, timeout=3, allow_redirects=True)
                    head_resp.raise_for_status()
                    content_type = head_resp.headers.get('Content-Type')
                    if content_type:
                        guessed_ext = mimetypes.guess_extension(content_type.split(';')[0])
                        if guessed_ext:
                            ext = guessed_ext
                except:
                    ext = ".png"
            
            # í•œê¸€ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
            korean_name = self.convert_to_korean(base_name)
            safe_name = self.sanitize_filename(korean_name) + ext
            
            # ì €ì¥ ê²½ë¡œ ê²°ì •
            save_dir = IMAGE_DIR / subfolder if subfolder else IMAGE_DIR
            save_path = save_dir / safe_name
            save_path = Path(self.get_unique_filename(str(save_path)))
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ë‹¤ìš´ë¡œë“œ
            response = requests.get(full_image_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            # ìƒëŒ€ ê²½ë¡œ ë°˜í™˜
            relative_path = save_path.relative_to(DATA_DIR).as_posix()
            return str(relative_path)
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {full_image_url}: {e}")
            return None
    
    def scrape_character_list(self):
        """ìºë¦­í„° ëª©ë¡ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        print("ğŸ“¡ ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        try:
            response = requests.get(TARGET_URL, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table', class_='wikitable')
            if not tables:
                print("âŒ ìºë¦­í„° í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            characters = []
            for table in tables:
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows:
                    cols = row.find_all(['td', 'th'])
                    if len(cols) < 2:
                        continue
                    
                    # ìºë¦­í„° ë§í¬ ë° ì´ë¦„ ì¶”ì¶œ
                    char_link = cols[0].find('a')
                    if not char_link:
                        continue
                    
                    eng_name = char_link.get('title', '').strip()
                    detail_url = urljoin(BASE_URL, char_link.get('href', ''))
                    
                    if not eng_name or not detail_url:
                        continue
                    
                    # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                    img_tag = cols[1].find('img')
                    img_url = img_tag.get('src', '') if img_tag else ''
                    
                    characters.append({
                        'english_name': eng_name,
                        'detail_url': detail_url,
                        'image_url': img_url
                    })
            
            print(f"âœ… ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(characters)}ê°œ")
            return characters
            
        except Exception as e:
            print(f"âŒ ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def scrape_character_details(self, detail_url, eng_name):
        """ìºë¦­í„° ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        try:
            response = requests.get(detail_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # infoboxì—ì„œ ë°ì´í„° ì¶”ì¶œ
            infobox = soup.find('table', class_='infobox')
            if not infobox:
                return {}
            
            data = {}
            rows = infobox.find_all('tr')
            
            for row in rows:
                header = row.find(['th', 'td'])
                if not header:
                    continue
                
                header_text = header.get_text(strip=True)
                if 'Rarity' in header_text or 'í¬ê·€ë„' in header_text:
                    rarity_cell = row.find_all(['td', 'th'])[-1]
                    data['rarity'] = rarity_cell.get_text(strip=True)
                elif 'Element' in header_text or 'ì†ì„±' in header_text:
                    element_cell = row.find_all(['td', 'th'])[-1]
                    data['elements'] = element_cell.get_text(strip=True)
                elif 'Weapon' in header_text or 'ë¬´ê¸°' in header_text:
                    weapon_cell = row.find_all(['td', 'th'])[-1]
                    data['weapons'] = weapon_cell.get_text(strip=True)
            
            # ê³ í™”ì§ˆ ì´ë¯¸ì§€ ì¶”ì¶œ
            img_tag = infobox.find('img')
            if img_tag:
                img_src = img_tag.get('src', '')
                if img_src.startswith('http'):
                    data['high_res_image_url'] = img_src
                else:
                    data['high_res_image_url'] = urljoin(BASE_URL, img_src)
            
            return data
            
        except Exception as e:
            print(f"âš ï¸ {eng_name} ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def scrape_all_personalities(self):
        """ì „ì²´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘"""
        print("ğŸ­ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì¤‘...")
        try:
            response = requests.get(PERSONALITY_URL, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # wikitable í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ ì°¾ê¸°
            tables = soup.find_all('table', class_='wikitable')
            character_personalities = {}
            
            for table in tables:
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        # í¼ìŠ¤ë„ë¦¬í‹° ì´ë¦„
                        personality_cell = cells[0]
                        personality_eng = personality_cell.get_text(strip=True)
                        personality_kor = self.personality_mapping.get(personality_eng, personality_eng)
                        
                        # ìºë¦­í„° ëª©ë¡
                        characters_cell = cells[1]
                        character_links = characters_cell.find_all('a')
                        characters_found = []
                        
                        for link in character_links:
                            href = link.get('href', '')
                            if (href.startswith('/w/') and 
                                'Character' not in href and 
                                'Personality' not in href and
                                'Special:' not in href and
                                'Category:' not in href):
                                
                                char_name = link.get_text(strip=True)
                                if char_name and len(char_name) > 1:
                                    characters_found.append(char_name)
                        
                        # ê° ìºë¦­í„°ì— í¼ìŠ¤ë„ë¦¬í‹° ì¶”ê°€
                        for char_name in characters_found:
                            if char_name not in character_personalities:
                                character_personalities[char_name] = []
                            character_personalities[char_name].append(personality_kor)
            
            print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(character_personalities)}ëª…")
            return character_personalities
            
        except Exception as e:
            print(f"âŒ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def create_excel_with_images(self, characters):
        """ì´ë¯¸ì§€ í¬í•¨ ì—‘ì…€ íŒŒì¼ ìƒì„±"""
        print("ğŸ“Š ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘...")
        try:
            wb = Workbook()
            ws = wb.active
            ws.title = "Characters"
            
            # í—¤ë”
            headers = ['English Name', 'Korean Name', 'Rarity', 'Elements', 'Weapons', 'Personalities', 'Image Path']
            ws.append(headers)
            
            # ë°ì´í„° ì¶”ê°€
            for char in characters:
                row = [
                    char.get('english_name', ''),
                    char.get('korean_name', ''),
                    char.get('rarity', ''),
                    char.get('elements', ''),
                    char.get('weapons', ''),
                    char.get('personalities', ''),
                    char.get('image_path', '')
                ]
                ws.append(row)
            
            # ì—´ ë„ˆë¹„ ì¡°ì •
            for column in ws.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws.column_dimensions[column_letter].width = adjusted_width
            
            # íŒŒì¼ ì €ì¥
            excel_path = CSV_DIR / "another_eden_characters_detailed.xlsx"
            wb.save(excel_path)
            print(f"âœ… ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {excel_path}")
            return excel_path
            
        except Exception as e:
            print(f"âŒ ì—‘ì…€ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_csv_files(self, characters, personality_data):
        """CSV íŒŒì¼ë“¤ ìƒì„±"""
        print("ğŸ“‹ CSV íŒŒì¼ë“¤ ìƒì„± ì¤‘...")
        
        # 1. í€´ì¦ˆìš© ë°ì´í„° (eden_quiz_data.csv)
        quiz_data = []
        for char in characters:
            quiz_data.append({
                'ìºë¦­í„°ëª…': char.get('korean_name', ''),
                'ì˜ë¬¸ëª…': char.get('english_name', ''),
                'ìºë¦­í„°ì•„ì´ì½˜ê²½ë¡œ': char.get('image_path', ''),
                'í¬ê·€ë„': char.get('rarity', ''),
                'ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸': char.get('elements', ''),
                'ë¬´ê¸°ëª…ë¦¬ìŠ¤íŠ¸': char.get('weapons', ''),
                'ì„±ê²©íŠ¹ì„±ë¦¬ìŠ¤íŠ¸': char.get('personalities', '')
            })
        
        quiz_df = pd.DataFrame(quiz_data)
        quiz_csv_path = CSV_DIR / "eden_quiz_data.csv"
        quiz_df.to_csv(quiz_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í€´ì¦ˆ ë°ì´í„° ì €ì¥: {quiz_csv_path}")
        
        # 2. ë£°ë ›ìš© ë°ì´í„° (eden_roulette_data.csv)
        roulette_data = []
        for char in characters:
            roulette_data.append({
                'english_name': char.get('english_name', ''),
                'korean_name': char.get('korean_name', ''),
                'image_path': char.get('image_path', ''),
                'rarity': char.get('rarity', ''),
                'elements': char.get('elements', ''),
                'weapons': char.get('weapons', ''),
                'personalities': char.get('personalities', '')
            })
        
        roulette_df = pd.DataFrame(roulette_data)
        roulette_csv_path = CSV_DIR / "eden_roulette_data.csv"
        roulette_df.to_csv(roulette_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ë£°ë › ë°ì´í„° ì €ì¥: {roulette_csv_path}")
        
        # 3. í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° (character_personalities.csv)
        personality_list = []
        for eng_name, personalities in personality_data.items():
            kor_name = self.name_mapping.get(eng_name.lower(), eng_name)
            personality_list.append({
                'English_Name': eng_name,
                'Korean_Name': kor_name,
                'Personalities_Korean': ', '.join(personalities),
                'Personalities_Count': len(personalities),
                'Personalities_List': '|'.join(personalities)
            })
        
        personality_df = pd.DataFrame(personality_list)
        personality_df.sort_values('Personalities_Count', ascending=False, inplace=True)
        personality_csv_path = CSV_DIR / "character_personalities.csv"
        personality_df.to_csv(personality_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ì €ì¥: {personality_csv_path}")
        
        return quiz_csv_path, roulette_csv_path, personality_csv_path
    
    def run_full_scraping(self):
        """ì „ì²´ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print("ğŸš€ Another Eden í†µí•© ìŠ¤í¬ë˜í¼ ì‹œì‘")
        print("=" * 60)
        
        # ë§¤í•‘ ë¡œë“œ
        self.load_name_mapping()
        self.load_personality_mapping()
        
        # 1. ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘
        characters = self.scrape_character_list()
        if not characters:
            print("âŒ ìºë¦­í„° ë°ì´í„° ì—†ìŒ")
            return False
        
        # 2. í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘
        personality_data = self.scrape_all_personalities()
        
        # 3. ê° ìºë¦­í„° ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘
        print("ğŸ“„ ìºë¦­í„° ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        for i, char in enumerate(characters, 1):
            eng_name = char['english_name']
            print(f"[{i}/{len(characters)}] {eng_name} ì²˜ë¦¬ ì¤‘...")
            
            # í•œê¸€ëª… ë³€í™˜
            kor_name = self.convert_to_korean(eng_name)
            char['korean_name'] = kor_name
            
            # ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘
            details = self.scrape_character_details(char['detail_url'], eng_name)
            
            # í¬ê·€ë„, ì†ì„±, ë¬´ê¸° ì •ë³´
            char['rarity'] = details.get('rarity', '')
            char['elements'] = details.get('elements', '')
            char['weapons'] = details.get('weapons', '')
            
            # í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´
            personalities = personality_data.get(eng_name, [])
            char['personalities'] = ', '.join(personalities)
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            image_url = details.get('high_res_image_url') or char['image_url']
            if image_url:
                image_path = self.download_image(image_url)
                char['image_path'] = image_path or ''
            else:
                char['image_path'] = ''
            
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        # 4. ì—‘ì…€ ë° CSV íŒŒì¼ ìƒì„±
        excel_path = self.create_excel_with_images(characters)
        csv_paths = self.generate_csv_files(characters, personality_data)
        
        print("\nğŸ‰ í†µí•© ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì´ ìºë¦­í„° ìˆ˜: {len(characters)}")
        if excel_path:
            print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼: {excel_path}")
        print(f"ğŸ’¾ CSV íŒŒì¼ë“¤: {', '.join(map(str, csv_paths))}")
        
        return True


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = MasterScraper()
    success = scraper.run_full_scraping()
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
