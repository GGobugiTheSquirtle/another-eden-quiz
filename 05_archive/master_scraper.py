#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Another Eden í†µí•© ìŠ¤í¬ë˜í¼
ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ìë™ ëŒ€ì‘í•˜ëŠ” í†µí•© ë°ì´í„° ìƒì„±ê¸°
"""

import os
import sys
import re
import json
import time
import requests
import pandas as pd
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote, parse_qs
from bs4 import BeautifulSoup
import unicodedata
from datetime import datetime

# ìŠ¤í¬ë˜í•‘ ì„¤ì •
BASE_URL = "https://anothereden.wiki"
TARGET_URL = "https://anothereden.wiki/w/Characters"
PERSONALITY_URL = "https://anothereden.wiki/w/Characters/Personality"

# ì„¸ì…˜ ì„¤ì • (ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬)
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
})

class UnifiedScraper:
    """í†µí•© Another Eden ìŠ¤í¬ë˜í¼ (ìë™ ëŒ€ì‘í˜•)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.project_root = Path(__file__).parent.parent.resolve()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.character_data = []
        self.personality_data = {}
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.setup_directories()
        
        # ë§¤í•‘ ë°ì´í„° ë¡œë“œ
        self.load_name_mapping()
        self.load_personality_mapping()
    
    def setup_directories(self):
        """ë””ë ‰í† ë¦¬ ì„¤ì •"""
        self.csv_dir = self.project_root / "04_data" / "csv"
        self.image_dir = self.project_root / "04_data" / "images" / "character_art"
        self.icon_dir = self.project_root / "04_data" / "images" / "character_art" / "icons"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [self.csv_dir, self.image_dir, self.icon_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        print("âœ… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ")
        print(f"   ğŸ“Š ë°ì´í„°: {self.csv_dir}")
        print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€: {self.image_dir}")
    
    def load_name_mapping(self):
        """í•œê¸€ ì´ë¦„ ë§¤í•‘ ë¡œë“œ"""
        mapping_file = self.project_root / "04_data" / "csv" / "Matching_names.csv"
        if mapping_file.exists():
            try:
                df = pd.read_csv(mapping_file)
                # ì»¬ëŸ¼ëª… í™•ì¸ ë° ë§¤í•‘
                if 'English' in df.columns and 'Korean' in df.columns:
                    self.name_mapping = dict(zip(df['English'], df['Korean']))
                elif 'ìºë¦­í„°ëª… (ì…ë ¥)' in df.columns and 'ìºë¦­í„°ëª… (ë§¤ì¹­)' in df.columns:
                    self.name_mapping = dict(zip(df['ìºë¦­í„°ëª… (ì…ë ¥)'], df['ìºë¦­í„°ëª… (ë§¤ì¹­)']))
                else:
                    # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
                    self.name_mapping = dict(zip(df.iloc[:, 0], df.iloc[:, 1]))
                
                print(f"âœ… í•œê¸€ ë§¤ì¹­ ë¡œë“œ: {len(self.name_mapping)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ ë§¤ì¹­ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.name_mapping = {}
        else:
            print("âš ï¸ ë§¤ì¹­ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            self.name_mapping = {}
    
    def load_personality_mapping(self):
        """í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ"""
        mapping_file = self.project_root / "04_data" / "csv" / "personality_matching.csv"
        if mapping_file.exists():
            try:
                df = pd.read_csv(mapping_file)
                self.personality_mapping = dict(zip(df['English'], df['Korean']))
                print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ: {len(self.personality_mapping)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.personality_mapping = {}
        else:
            print("âš ï¸ í¼ìŠ¤ë„ë¦¬í‹° ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            self.personality_mapping = {}
    
    def load_existing_personality_data(self):
        """ê¸°ì¡´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ë¡œë“œ (ë ˆê±°ì‹œ ìŠ¤í¬ë˜í¼ ê²°ê³¼ í™œìš©)"""
        personality_file = Path("character_personalities.csv")
        if personality_file.exists():
            try:
                df = pd.read_csv(personality_file)
                personality_data = {}
                
                for _, row in df.iterrows():
                    eng_name = row['English_Name']
                    personalities = row['Personalities_List'].split('|')
                    personality_data[eng_name] = personalities
                
                print(f"âœ… ê¸°ì¡´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ë¡œë“œ: {len(personality_data)}ëª…")
                return personality_data
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        else:
            print("âš ï¸ ê¸°ì¡´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {}
    
    def convert_to_korean(self, english_name):
        """ì˜ì–´ ì´ë¦„ì„ í•œê¸€ë¡œ ë³€í™˜"""
        if english_name in self.name_mapping:
            return self.name_mapping[english_name]
        return english_name
    
    def sanitize_filename(self, name):
        """íŒŒì¼ëª… ì •ë¦¬"""
        sanitized = re.sub(r'[<>:"/\\|?*]', '', name)
        sanitized = sanitized.replace(' ', '_')
        sanitized = sanitized.replace('&', 'and')
        return sanitized
    
    def get_unique_filename(self, filepath):
        """ì¤‘ë³µë˜ì§€ ì•ŠëŠ” íŒŒì¼ëª… ìƒì„±"""
        if not filepath.exists():
            return filepath
        
        stem = filepath.stem
        suffix = filepath.suffix
        counter = 1
        
        while filepath.exists():
            new_name = f"{stem}_{counter}{suffix}"
            filepath = filepath.parent / new_name
            counter += 1
        
        return filepath
    
    def download_image(self, image_url, kor_name="", eng_name=""):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ë ˆê±°ì‹œ ë°©ì‹ ì°¸ì¡°)"""
        if not image_url or image_url.startswith('data:'):
            return ""
        
        try:
            # URL ì •ê·œí™”
            if image_url.startswith('//'):
                image_url = 'https:' + image_url
            elif image_url.startswith('/'):
                image_url = BASE_URL + image_url
            
            # ë ˆê±°ì‹œ ë°©ì‹ì˜ íŒŒì¼ëª… ìƒì„± ë¡œì§
            parsed_url = urlparse(image_url)
            query_params = parse_qs(parsed_url.query)
            image_name_from_f = query_params.get('f', [None])[0]
            
            if image_name_from_f:
                image_name = os.path.basename(unquote(image_name_from_f))
            else:
                image_name = os.path.basename(unquote(parsed_url.path.split('?')[0]))

            if not image_name or image_name.lower() in ["thumb.php", "index.php"]:
                temp_name = unquote(image_url.split('/')[-1].split('?')[0])
                image_name = (temp_name[:50] + ".png") if temp_name else "unknown_image.png"

            base_name, ext = os.path.splitext(image_name)
            if not ext or len(ext) > 5:
                try:
                    head_resp = session.head(image_url, timeout=3, allow_redirects=True)
                    head_resp.raise_for_status()
                    content_type = head_resp.headers.get('Content-Type')
                    if content_type:
                        import mimetypes
                        guessed_ext = mimetypes.guess_extension(content_type.split(';')[0])
                        image_name = base_name + (guessed_ext if guessed_ext and guessed_ext != '.jpe' else ('.jpg' if guessed_ext == '.jpe' else '.png'))
                    else: 
                        image_name = base_name + ".png" 
                except: 
                    image_name = base_name + ".png"
            
            # íŒŒì¼ëª… ì •ë¦¬
            image_name = re.sub(r'[<>:"/\\|?*]', '_', image_name)
            image_name = image_name[:200]
            
            # ìºë¦­í„° ì´ë¯¸ì§€ëŠ” icons í´ë”ì— ì €ì¥
            save_path_dir = self.icon_dir
            save_path = save_path_dir / image_name
            save_path = self.get_unique_filename(save_path)
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ì²´í¬
            if save_path.exists() and save_path.stat().st_size > 0:
                print(f"   âœ… ì´ë¯¸ì§€ ì´ë¯¸ ì¡´ì¬: {save_path.name}")
                return str(save_path)
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = session.get(image_url, timeout=30, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(8192):
                    f.write(chunk)
            
            print(f"   ğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {save_path.name}")
            time.sleep(0.05)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            return str(save_path)
            
        except Exception as e:
            print(f"   âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""
    
    def download_icon(self, icon_url, alt_text, subfolder):
        """ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ (ë ˆê±°ì‹œ ë°©ì‹ ì°¸ì¡°)"""
        if not icon_url or icon_url.startswith('data:'):
            return ""
        
        try:
            # URL ì •ê·œí™”
            if icon_url.startswith('//'):
                icon_url = 'https:' + icon_url
            elif icon_url.startswith('/'):
                icon_url = BASE_URL + icon_url
            
            # ë ˆê±°ì‹œ ë°©ì‹ì˜ íŒŒì¼ëª… ìƒì„±
            parsed_url = urlparse(icon_url)
            query_params = parse_qs(parsed_url.query)
            image_name_from_f = query_params.get('f', [None])[0]
            
            if image_name_from_f:
                image_name = os.path.basename(unquote(image_name_from_f))
            else:
                image_name = os.path.basename(unquote(parsed_url.path.split('?')[0]))

            if not image_name or image_name.lower() in ["thumb.php", "index.php"]:
                temp_name = unquote(icon_url.split('/')[-1].split('?')[0])
                image_name = (temp_name[:50] + ".png") if temp_name else "unknown_icon.png"

            base_name, ext = os.path.splitext(image_name)
            if not ext or len(ext) > 5:
                try:
                    head_resp = session.head(icon_url, timeout=3, allow_redirects=True)
                    head_resp.raise_for_status()
                    content_type = head_resp.headers.get('Content-Type')
                    if content_type:
                        import mimetypes
                        guessed_ext = mimetypes.guess_extension(content_type.split(';')[0])
                        image_name = base_name + (guessed_ext if guessed_ext and guessed_ext != '.jpe' else ('.jpg' if guessed_ext == '.jpe' else '.png'))
                    else: 
                        image_name = base_name + ".png" 
                except: 
                    image_name = base_name + ".png"
            
            # íŒŒì¼ëª… ì •ë¦¬
            image_name = re.sub(r'[<>:"/\\|?*]', '_', image_name)
            image_name = image_name[:200]
            
            # ì„œë¸Œí´ë” ìƒì„±
            subfolder_path = self.icon_dir / subfolder
            subfolder_path.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ê²½ë¡œ
            icon_path = subfolder_path / image_name
            icon_path = self.get_unique_filename(icon_path)
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ì²´í¬
            if icon_path.exists() and icon_path.stat().st_size > 0:
                print(f"   âœ… ì•„ì´ì½˜ ì´ë¯¸ ì¡´ì¬: {icon_path.name}")
                return str(icon_path)
            
            # ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ
            response = session.get(icon_url, timeout=30, stream=True)
            response.raise_for_status()
            
            with open(icon_path, 'wb') as f:
                for chunk in response.iter_content(8192):
                    f.write(chunk)
            
            print(f"   ğŸ“¥ ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ: {icon_path.name}")
            time.sleep(0.05)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            return str(icon_path)
            
        except Exception as e:
            print(f"   âš ï¸ ì•„ì´ì½˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""
    
    def scrape_character_list_adaptive(self):
        """ì ì‘í˜• ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ë ˆê±°ì‹œ ë°©ì‹)"""
        print("ğŸ“¡ ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        
        try:
            response = session.get(TARGET_URL, timeout=60)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë ˆê±°ì‹œ ë°©ì‹: chara-table ë˜ëŠ” wikitable í´ë˜ìŠ¤ ì°¾ê¸°
            char_table = soup.find('table', class_='chara-table') or soup.find('table', class_='wikitable')
            
            if not char_table:
                print("âŒ ìºë¦­í„° í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            print("âœ… ìºë¦­í„° í…Œì´ë¸” ë°œê²¬. í–‰ íŒŒì‹± ì¤‘...")
            
            characters = []
            rows = char_table.find_all('tr')
            total_rows = len(rows) - 1  # í—¤ë” ì œì™¸
            
            print(f"ğŸ“Š ë°œê²¬ëœ ìºë¦­í„° í–‰: {total_rows}ê°œ")
            
            for i, row in enumerate(rows[1:], 1):  # í—¤ë” ì œì™¸
                cells = row.find_all('td')
                if len(cells) < 4:
                    continue
                
                try:
                    # ë ˆê±°ì‹œ ë°©ì‹: ì•„ì´ì½˜ ì…€ (ì²« ë²ˆì§¸ ì…€)
                    icon_cell = cells[0]
                    icon_img_tag = icon_cell.find('img')
                    icon_src = icon_img_tag.get('src', '') if icon_img_tag else ''
                    
                    # ë ˆê±°ì‹œ ë°©ì‹: ì´ë¦„/í¬ê·€ë„ ì…€ (ë‘ ë²ˆì§¸ ì…€)
                    name_rarity_cell = cells[1]
                    name_tag = name_rarity_cell.find('a')
                    
                    if not name_tag:
                        continue
                    
                    original_name = name_tag.text.strip()
                    detail_url = urljoin(BASE_URL, name_tag.get('href', ''))
                    
                    if not original_name or not detail_url:
                        continue
                    
                    # ë ˆê±°ì‹œ ë°©ì‹: í¬ê·€ë„ ì¶”ì¶œ
                    rarity = ""
                    lines_in_cell = [line.strip() for line in name_rarity_cell.get_text(separator='\n').splitlines() if line.strip()]
                    for line_text in reversed(lines_in_cell):
                        if "â˜…" in line_text:
                            rarity = line_text
                            break
                    if not rarity:
                        rarity_match = re.search(r'\d(?:~\d)?â˜…(?:\s*\S+)?', name_rarity_cell.get_text(separator=" ").strip())
                        if rarity_match:
                            rarity = rarity_match.group(0).strip()
                    
                    # ë ˆê±°ì‹œ ë°©ì‹: ì†ì„±/ì¥ë¹„ ì…€ (ì„¸ ë²ˆì§¸ ì…€)
                    element_equipment_cell = cells[2]
                    ee_icon_tags = element_equipment_cell.find_all('img')
                    element_equipment_icon_paths = []
                    element_equipment_icon_alts = []
                    
                    for img_tag in ee_icon_tags:
                        ee_src = img_tag.get('src')
                        ee_alt = img_tag.get('alt', "")
                        if ee_src:
                            local_path = self.download_icon(ee_src, ee_alt, "elements_equipment")
                            if local_path:
                                element_equipment_icon_paths.append(local_path)
                                element_equipment_icon_alts.append(ee_alt)
                    
                    # ì¶œì‹œì¼ ì…€ (ë„¤ ë²ˆì§¸ ì…€)
                    release_date = cells[3].text.strip() if len(cells) > 3 else ""
                    
                    characters.append({
                        'english_name': original_name,
                        'detail_url': detail_url,
                        'image_url': icon_src,
                        'rarity': rarity,
                        'element_equipment_icon_paths': element_equipment_icon_paths,
                        'element_equipment_icon_alts': element_equipment_icon_alts,
                        'release_date': release_date
                    })
                    
                    if i % 50 == 0:
                        print(f"   ğŸ“ ì²˜ë¦¬ ì¤‘: {i}/{total_rows}")
                        
                except Exception as e:
                    print(f"   âš ï¸ í–‰ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"âœ… ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(characters)}ê°œ")
            return characters
            
        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def scrape_from_tables(self, soup):
        """í…Œì´ë¸”ì—ì„œ ìºë¦­í„° ì¶”ì¶œ"""
        characters = []
        
        # ì•Œë ¤ì§„ í…Œì´ë¸” í´ë˜ìŠ¤ë“¤ ì‹œë„
        table_classes = ['chara-table', 'wikitable', 'sortable', 'character-table']
        
        for class_name in table_classes:
            tables = soup.find_all('table', class_=class_name)
            if tables:
                print(f"âœ… {class_name} í´ë˜ìŠ¤ í…Œì´ë¸” ë°œê²¬")
                
                for table in tables:
                    rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                    
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        if len(cells) < 2:
                            continue
                        
                        # ì´ë¦„ ë° ë§í¬ ì¶”ì¶œ
                        name_cell = cells[1] if len(cells) > 1 else cells[0]
                        name_tag = name_cell.find('a')
                        
                        if not name_tag:
                            continue
                        
                        original_name = name_tag.text.strip()
                        detail_url = urljoin(BASE_URL, name_tag.get('href', ''))
                        
                        if not original_name or not detail_url:
                            continue
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        icon_cell = cells[0] if len(cells) > 0 else None
                        image_url = ""
                        if icon_cell:
                            icon_img = icon_cell.find('img')
                            if icon_img:
                                image_url = icon_img.get('src', '')
                        
                        characters.append({
                            'english_name': original_name,
                            'detail_url': detail_url,
                            'image_url': image_url
                        })
                
                if characters:
                    return characters
        
        return []
    
    def scrape_from_links(self, soup):
        """ëª¨ë“  ë§í¬ì—ì„œ ìºë¦­í„° ì°¾ê¸°"""
        characters = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link['href']
            # ìºë¦­í„° í˜ì´ì§€ í•„í„°ë§
            if ('/w/' in href and 
                'Character' not in href and 
                'Special:' not in href and
                'Category:' not in href and
                'Template:' not in href and
                'User:' not in href and
                'Talk:' not in href and
                'Help:' not in href and
                'File:' not in href and
                'MediaWiki:' not in href):
                
                title = link.get('title', '') or link.get_text(strip=True)
                if title and len(title) > 2 and len(title) < 50:
                    characters.append({
                        'english_name': title,
                        'detail_url': urljoin(BASE_URL, href),
                        'image_url': ''
                    })
        
        # ì¤‘ë³µ ì œê±°
        unique_chars = []
        seen = set()
        for char in characters:
            if char['english_name'] not in seen:
                unique_chars.append(char)
                seen.add(char['english_name'])
        
        return unique_chars[:50]  # ìµœëŒ€ 50ê°œë§Œ
    
    def scrape_direct_characters(self):
        """ì§ì ‘ ìºë¦­í„° í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        # ì•Œë ¤ì§„ ìºë¦­í„° ì´ë¦„ë“¤ë¡œ ì§ì ‘ ì‹œë„
        known_characters = [
            'Aldo', 'Feinne', 'Riica', 'Cyrus', 'Amy', 'Mariel', 'Bivette',
            'Nagi', 'Mighty', 'Toova', 'Lokido', 'Anabel', 'Suzette', 'Isuka'
        ]
        
        characters = []
        for char_name in known_characters:
            try:
                char_url = f"{BASE_URL}/w/{char_name}"
                response = session.get(char_url, timeout=10)
                if response.status_code == 200:
                    characters.append({
                        'english_name': char_name,
                        'detail_url': char_url,
                        'image_url': ''
                    })
                    print(f"   âœ… {char_name} í˜ì´ì§€ ë°œê²¬")
            except:
                continue
        
        return characters
    
    def scrape_character_details(self, detail_url, eng_name):
        """ìºë¦­í„° ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ (ë ˆê±°ì‹œ ë°©ì‹ ì°¸ì¡°)"""
        try:
            response = session.get(detail_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            data = {}
            element_icons = []
            weapon_icons = []
            
            # ë ˆê±°ì‹œ ë°©ì‹: ëª¨ë“  í…Œì´ë¸”ì—ì„œ ì •ë³´ ì°¾ê¸°
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows:
                    cells = row.find_all(['th', 'td'])
                    if len(cells) < 2:
                        continue
                    
                    header_cell = cells[0]
                    value_cell = cells[-1] if len(cells) > 1 else None
                    
                    if not header_cell or not value_cell:
                        continue
                    
                    header_text = header_cell.get_text(strip=True).lower()
                    value_text = value_cell.get_text(strip=True)
                    
                    # ë ˆê±°ì‹œ ë°©ì‹: í¬ê·€ë„ ì¶”ì¶œ
                    if any(keyword in header_text for keyword in ['rarity', 'í¬ê·€ë„', 'star', 'ë³„']):
                        # ë ˆê±°ì‹œ ë°©ì‹: â˜… íŒ¨í„´ ì°¾ê¸°
                        rarity_match = re.search(r'\d(?:~\d)?â˜…(?:\s*\S+)?', value_text)
                        if rarity_match:
                            data['rarity'] = rarity_match.group(0).strip()
                        elif 'â˜…' in value_text:
                            data['rarity'] = value_text
                        elif 'SA' in value_text.upper() or 'Stellar Awakened' in value_text:
                            data['rarity'] = "5â˜… ì„±ë„ê°ì„±"
                    
                    # ë ˆê±°ì‹œ ë°©ì‹: ì†ì„± ì¶”ì¶œ
                    elif any(keyword in header_text for keyword in ['element', 'ì†ì„±', 'type']):
                        if value_text and value_text.lower() != 'n/a':
                            data['elements'] = value_text
                            
                            # ì†ì„± ì•„ì´ì½˜ ì°¾ê¸° (ë ˆê±°ì‹œ ë°©ì‹)
                            img_tags = value_cell.find_all('img')
                            for img in img_tags:
                                src = img.get('src', '')
                                alt = img.get('alt', '').lower()
                                if src and any(element in alt for element in ['fire', 'water', 'earth', 'wind', 'light', 'dark', 'crystal']):
                                    icon_path = self.download_icon(src, alt, "elements_equipment")
                                    if icon_path:
                                        element_icons.append(icon_path)
                    
                    # ë ˆê±°ì‹œ ë°©ì‹: ë¬´ê¸° ì¶”ì¶œ
                    elif any(keyword in header_text for keyword in ['weapon', 'ë¬´ê¸°', 'arms']):
                        if value_text and value_text.lower() != 'n/a':
                            data['weapons'] = value_text
                            
                            # ë¬´ê¸° ì•„ì´ì½˜ ì°¾ê¸° (ë ˆê±°ì‹œ ë°©ì‹)
                            img_tags = value_cell.find_all('img')
                            for img in img_tags:
                                src = img.get('src', '')
                                alt = img.get('alt', '').lower()
                                if src and any(weapon in alt for weapon in ['sword', 'katana', 'axe', 'hammer', 'spear', 'bow', 'staff', 'fist']):
                                    icon_path = self.download_icon(src, alt, "elements_equipment")
                                    if icon_path:
                                        weapon_icons.append(icon_path)
            
            # ë ˆê±°ì‹œ ë°©ì‹: ê³ í™”ì§ˆ ì´ë¯¸ì§€ ì°¾ê¸°
            img_tag = soup.find('img', class_='thumbimage') or soup.find('img', class_='infobox-image')
            if img_tag:
                img_src = img_tag.get('src', '')
                if img_src.startswith('http'):
                    data['high_res_image_url'] = img_src
                else:
                    data['high_res_image_url'] = urljoin(BASE_URL, img_src)
            
            # ì•„ì´ì½˜ ì •ë³´ ì¶”ê°€
            data['element_icons'] = element_icons
            data['weapon_icons'] = weapon_icons
            
            # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
            if data:
                print(f"  ğŸ“Š {eng_name}: í¬ê·€ë„={data.get('rarity', 'N/A')}, ì†ì„±={data.get('elements', 'N/A')}, ë¬´ê¸°={data.get('weapons', 'N/A')}")
                if element_icons:
                    print(f"    ğŸ¯ ì†ì„± ì•„ì´ì½˜: {len(element_icons)}ê°œ")
                if weapon_icons:
                    print(f"    âš”ï¸ ë¬´ê¸° ì•„ì´ì½˜: {len(weapon_icons)}ê°œ")
            
            return data
            
        except Exception as e:
            print(f"âš ï¸ {eng_name} ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def scrape_personalities(self):
        """í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´ ìŠ¤í¬ë˜í•‘ (ë ˆê±°ì‹œ ë°©ì‹ ì°¸ì¡°)"""
        print("ğŸ­ í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        try:
            response = session.get(PERSONALITY_URL, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            character_personalities = {}
            
            # ë ˆê±°ì‹œ ë°©ì‹: wikitable í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ ì°¾ê¸°
            tables = soup.find_all('table', class_='wikitable')
            print(f"ğŸ“Š ë°œê²¬ëœ í¼ìŠ¤ë„ë¦¬í‹° í…Œì´ë¸” ìˆ˜: {len(tables)}")
            
            personality_count = 0
            character_total = 0
            
            for table_idx, table in enumerate(tables):
                print(f"ğŸ” í¼ìŠ¤ë„ë¦¬í‹° í…Œì´ë¸” {table_idx + 1} ì²˜ë¦¬ ì¤‘...")
                
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        # ì²« ë²ˆì§¸ ì…€: í¼ìŠ¤ë„ë¦¬í‹° ì´ë¦„
                        personality_cell = cells[0]
                        personality_eng = personality_cell.get_text(strip=True)
                        
                        # í•œêµ­ì–´ í¼ìŠ¤ë„ë¦¬í‹°ëª…ìœ¼ë¡œ ë³€í™˜
                        personality_kor = self.personality_mapping.get(personality_eng, personality_eng)
                        
                        # ë‘ ë²ˆì§¸ ì…€: ìºë¦­í„° ëª©ë¡
                        characters_cell = cells[1]
                        
                        # ìºë¦­í„° ë§í¬ë“¤ ì¶”ì¶œ
                        character_links = characters_cell.find_all('a', href=True)
                        characters_found = []
                        
                        for link in character_links:
                            href = link.get('href', '')
                            # ìºë¦­í„° í˜ì´ì§€ì¸ì§€ í™•ì¸ (/w/ë¡œ ì‹œì‘í•˜ê³  íŠ¹ì • íŒ¨í„´ ì œì™¸)
                            if (href.startswith('/w/') and 
                                'Character' not in href and 
                                'Personality' not in href and
                                'Special:' not in href and
                                'Category:' not in href and
                                'Template:' not in href and
                                'User:' not in href and
                                'Talk:' not in href and
                                'Help:' not in href and
                                'File:' not in href and
                                'MediaWiki:' not in href):
                                
                                char_name = link.get_text(strip=True)
                                if char_name and len(char_name) > 1:
                                    characters_found.append(char_name)
                        
                        if characters_found:
                            personality_count += 1
                            character_total += len(characters_found)
                            
                            print(f"  ğŸ­ {personality_kor} ({personality_eng}): {len(characters_found)}ëª…")
                            
                            # ê° ìºë¦­í„°ì— í¼ìŠ¤ë„ë¦¬í‹° ì¶”ê°€
                            for char_name in characters_found:
                                if char_name not in character_personalities:
                                    character_personalities[char_name] = []
                                character_personalities[char_name].append(personality_kor)
                            
                            # ì²˜ìŒ ëª‡ ê°œë§Œ ìƒì„¸ ì¶œë ¥
                            if personality_count <= 5:
                                sample_chars = characters_found[:5]
                                print(f"     â””â”€ ìƒ˜í”Œ: {', '.join(sample_chars)}")
            
            print(f"âœ… í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ í¼ìŠ¤ë„ë¦¬í‹° ìˆ˜: {personality_count}ê°œ")
            print(f"ğŸ­ ì´ ìºë¦­í„° ìˆ˜: {len(character_personalities)}ëª…")
            print(f"ğŸ“ˆ ì´ í¼ìŠ¤ë„ë¦¬í‹° ì—°ê²°: {character_total}ê°œ")
            
            return character_personalities
            
        except Exception as e:
            print(f"âŒ í¼ìŠ¤ë„ë¦¬í‹° ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def create_unified_csv(self, characters, personality_data):
        """í†µí•© CSV íŒŒì¼ ìƒì„±"""
        print("ğŸ“Š í†µí•© CSV íŒŒì¼ ìƒì„± ì¤‘...")
        
        unified_data = []
        
        for char in characters:
            # ê¸°ë³¸ ì •ë³´
            korean_name = self.convert_to_korean(char.get('english_name', ''))
            
            # í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´
            base_eng_name = re.sub(r'\s*\(.*\)$', '', char.get('english_name', '')).strip()
            personalities = personality_data.get(base_eng_name, [])
            korean_personalities = []
            for personality in personalities:
                korean_personality = self.personality_mapping.get(personality, personality)
                korean_personalities.append(korean_personality)
            
            # ì•„ì´ì½˜ ê²½ë¡œ ì²˜ë¦¬
            element_icons = char.get('element_equipment_icon_paths', [])
            weapon_icons = char.get('weapon_icons', [])
            
            # í†µí•© ë°ì´í„° êµ¬ì¡°
            unified_row = {
                'ìºë¦­í„°ëª…': korean_name,
                'English_Name': char.get('english_name', ''),
                'ìºë¦­í„°ì•„ì´ì½˜ê²½ë¡œ': char.get('image_url', ''),
                'í¬ê·€ë„': char.get('rarity', ''),
                'ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸': char.get('elements', ''),
                'ë¬´ê¸°ëª…ë¦¬ìŠ¤íŠ¸': char.get('weapons', ''),
                'í¼ìŠ¤ë„ë¦¬í‹°ë¦¬ìŠ¤íŠ¸': ', '.join(korean_personalities),
                'ì†ì„±_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸': '|'.join(element_icons),
                'ë¬´ê¸°_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸': '|'.join(weapon_icons),
                'ë°©ì–´êµ¬_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸': ''   # ë‚˜ì¤‘ì— ì¶”ê°€
            }
            
            unified_data.append(unified_row)
        
        # í†µí•© CSV ìƒì„±
        df = pd.DataFrame(unified_data)
        unified_csv_path = self.csv_dir / "eden_unified_data.csv"
        df.to_csv(unified_csv_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… í†µí•© CSV ìƒì„± ì™„ë£Œ: {unified_csv_path}")
        return unified_csv_path
    
    def run_unified_scraping(self):
        """í†µí•© ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print("ğŸš€ Another Eden í†µí•© ìŠ¤í¬ë˜í¼ ì‹œì‘")
        print("=" * 60)
        
        # 1. ì ì‘í˜• ìºë¦­í„° ëª©ë¡ ìŠ¤í¬ë˜í•‘
        characters = self.scrape_character_list_adaptive()
        if not characters:
            print("âŒ ìºë¦­í„° ë°ì´í„° ì—†ìŒ")
            return False
        
        print(f"ğŸ“Š ë°œê²¬ëœ ìºë¦­í„°: {len(characters)}ê°œ")
        
        # 2. í¼ìŠ¤ë„ë¦¬í‹° ì •ë³´ ë¡œë“œ (ê¸°ì¡´ ë°ì´í„° í™œìš©)
        personality_data = self.load_existing_personality_data()
        if not personality_data:
            print("ğŸ”„ ê¸°ì¡´ í¼ìŠ¤ë„ë¦¬í‹° ë°ì´í„°ê°€ ì—†ì–´ ìƒˆë¡œ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤...")
            personality_data = self.scrape_personalities()
        
        # 3. ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ (ì²˜ìŒ 10ê°œë§Œ í…ŒìŠ¤íŠ¸)
        print(f"ğŸ“¡ {min(10, len(characters))}ê°œ ìºë¦­í„° ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        
        for i, char in enumerate(characters[:10], 1):
            print(f"[{i}/{min(10, len(characters))}] {char['english_name']}")
            
            # ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘
            details = self.scrape_character_details(char['detail_url'], char['english_name'])
            char.update(details)
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            image_url = char.get('high_res_image_url') or char.get('image_url')
            if image_url:
                image_path = self.download_image(image_url, char.get('korean_name', ''), char['english_name'])
                char['image_url'] = image_path # image_pathë¥¼ image_urlë¡œ ë³€ê²½
            
            # í•œê¸€ ì´ë¦„ ì¶”ê°€
            char['korean_name'] = self.convert_to_korean(char['english_name'])
            
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        # 4. í†µí•© CSV ìƒì„±
        unified_csv_path = self.create_unified_csv(characters, personality_data)
        
        print("\nğŸ‰ í†µí•© ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ìºë¦­í„°: {len(characters)}ê°œ")
        print(f"ğŸ’¾ í†µí•© CSV: {unified_csv_path}")
        
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = UnifiedScraper()
    success = scraper.run_unified_scraping()
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
