"""
ğŸ”§ Another Eden Personality ë°ì´í„° ìŠ¤í¬ë ˆì´í¼
ê¸°ì¡´ ìºë¦­í„° ë°ì´í„°ì— Personalities ì •ë³´ë¥¼ ì¶”ê°€í•˜ëŠ” ê°œì„ ëœ ìŠ¤í¬ë ˆì´í¼
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import threading
import queue
import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.drawing.image import Image as OpenpyxlImage
from openpyxl.utils import get_column_letter
from urllib.parse import urljoin, unquote, parse_qs, urlparse
import os
import time
import re
# import csv  # unused in current implementation
import mimetypes
import configparser
import sys
import pandas as pd

# --- ê¸°ë³¸ ì„¤ì • ---
BASE_URL = "https://anothereden.wiki"
TARGET_URL = "https://anothereden.wiki/w/Characters"
PERSONALITY_URL = "https://anothereden.wiki/w/Characters/Personality"
CONFIG_FILE = "scraper_config.ini"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_BASE_DIR = SCRIPT_DIR
IMAGE_DIR = os.path.join(OUTPUT_BASE_DIR, "character_art")
STRUCTURE_ANALYSIS_FILENAME_CSV = os.path.join(OUTPUT_BASE_DIR, "structure_analysis.csv")
EXCEL_FILENAME_BASE = "another_eden_characters_detailed"

# --- ì„¤ì • íŒŒì¼ ì²˜ë¦¬ ---
def load_config():
    global OUTPUT_BASE_DIR, IMAGE_DIR, STRUCTURE_ANALYSIS_FILENAME_CSV
    config = configparser.ConfigParser()
    config_path = os.path.join(SCRIPT_DIR, CONFIG_FILE)
    if os.path.exists(config_path):
        config.read(config_path)
        OUTPUT_BASE_DIR = config.get('Paths', 'OutputDirectory', fallback=SCRIPT_DIR)
    else:
        OUTPUT_BASE_DIR = SCRIPT_DIR
    
    IMAGE_DIR = os.path.join(OUTPUT_BASE_DIR, "character_art")
    STRUCTURE_ANALYSIS_FILENAME_CSV = os.path.join(OUTPUT_BASE_DIR, "structure_analysis.csv")

def save_config():
    global OUTPUT_BASE_DIR
    config = configparser.ConfigParser()
    config['Paths'] = {'OutputDirectory': OUTPUT_BASE_DIR}
    config_path = os.path.join(SCRIPT_DIR, CONFIG_FILE)
    with open(config_path, 'w') as configfile:
        config.write(configfile)

def setup_directories():
    if not os.path.exists(IMAGE_DIR):
        os.makedirs(IMAGE_DIR)
    icons_path = os.path.join(IMAGE_DIR, "icons")
    elements_path = os.path.join(IMAGE_DIR, "elements_equipment")
    if not os.path.exists(icons_path):
        os.makedirs(icons_path)
    if not os.path.exists(elements_path):
        os.makedirs(elements_path)

def get_unique_filename(filepath):
    if not os.path.exists(filepath):
        return filepath
    base, ext = os.path.splitext(filepath)
    counter = 1
    while True:
        new_filepath = f"{base} ({counter}){ext}"
        if not os.path.exists(new_filepath):
            return new_filepath
        counter += 1

def download_image(image_url, subfolder=""):
    if not image_url:
        return None
    full_image_url = urljoin(BASE_URL, image_url)
    try:
        parsed_url = urlparse(full_image_url)
        query_params = parse_qs(parsed_url.query)
        image_name_from_f = query_params.get('f', [None])[0]
        
        if image_name_from_f:
            image_name = os.path.basename(unquote(image_name_from_f))
        else:
            image_name = os.path.basename(unquote(parsed_url.path.split('?')[0]))

        if not image_name or image_name.lower() in ["thumb.php", "index.php"]:
            temp_name = unquote(full_image_url.split('/')[-1].split('?')[0])
            image_name = (temp_name[:50] + ".png") if temp_name else "unknown_image.png"

        base_name, ext = os.path.splitext(image_name)
        if not ext or len(ext) > 5:
            try:
                head_resp = requests.head(full_image_url, timeout=3, allow_redirects=True)
                head_resp.raise_for_status()
                content_type = head_resp.headers.get('Content-Type')
                if content_type:
                    guessed_ext = mimetypes.guess_extension(content_type.split(';')[0])
                    image_name = base_name + (guessed_ext if guessed_ext and guessed_ext != '.jpe' else '.jpg')
                else: 
                    image_name = base_name + ".png" 
            except Exception: 
                image_name = base_name + ".png"
        
        image_name = re.sub(r'[<>:"/\\|?*]', '_', image_name)[:200]

        save_path_dir = os.path.join(IMAGE_DIR, subfolder)
        save_path = os.path.join(save_path_dir, image_name)
        
        if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
            return save_path

        img_response = requests.get(full_image_url, stream=True, timeout=10)
        img_response.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in img_response.iter_content(8192):
                f.write(chunk)
        time.sleep(0.05) 
        return save_path
    except requests.exceptions.RequestException as e:
        error_log_queue = getattr(threading.current_thread(), 'log_queue_ref', None)
        if error_log_queue: 
            error_log_queue.put(f"Download Error (Network) for {full_image_url.split('/')[-1][:30]}...: {type(e).__name__}")
    except Exception as e:
        error_log_queue = getattr(threading.current_thread(), 'log_queue_ref', None)
        if error_log_queue: 
            error_log_queue.put(f"Download Error (Other) for {full_image_url.split('/')[-1][:30]}...: {type(e).__name__}")
    return None

# --- í¼ìŠ¤ë„ë¦¬í‹° ë§¤ì¹­ ë°ì´í„° ë¡œë“œ ---
def load_personality_matching():
    """í¼ìŠ¤ë„ë¦¬í‹° ë§¤ì¹­ CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    personality_mapping = {}
    try:
        personality_csv_path = os.path.join(SCRIPT_DIR, "personality_matching.csv")
        if os.path.exists(personality_csv_path):
            df = pd.read_csv(personality_csv_path)
            for _, row in df.iterrows():
                if 'English' in row and 'Korean' in row:
                    personality_mapping[row['English']] = row['Korean']
    except Exception as e:
        print(f"Error loading personality matching: {e}")
    return personality_mapping

def scrape_all_personalities_from_page(headers_ua, log_queue_ref):
    """
    Characters/Personality í˜ì´ì§€ì—ì„œ ì „ì²´ í¼ìŠ¤ë„ë¦¬í‹° ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Returns:
        dict: {character_name: [personalities]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    personality_mapping = load_personality_matching()
    name_mapping = load_character_name_matching()
    character_personalities = {}
    
    try:
        log_queue_ref.put(f"Fetching personality data from: {PERSONALITY_URL}")
        response = requests.get(PERSONALITY_URL, headers=headers_ua, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # wikitable í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ ì°¾ê¸°
        tables = soup.find_all('table', class_='wikitable')
        log_queue_ref.put(f"Found {len(tables)} personality tables")
        
        for table_idx, table in enumerate(tables):
            rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    # ì²« ë²ˆì§¸ ì…€: í¼ìŠ¤ë„ë¦¬í‹° ì´ë¦„
                    personality_cell = cells[0]
                    personality_eng = personality_cell.get_text(strip=True)
                    
                    # í•œêµ­ì–´ í¼ìŠ¤ë„ë¦¬í‹°ëª…ìœ¼ë¡œ ë³€í™˜
                    personality_kor = personality_mapping.get(personality_eng, personality_eng)
                    
                    # ë‘ ë²ˆì§¸ ì…€: ìºë¦­í„° ëª©ë¡
                    characters_cell = cells[1]
                    
                    # ìºë¦­í„° ë§í¬ë“¤ ì¶”ì¶œ
                    character_links = characters_cell.find_all('a', href=True)
                    
                    for link in character_links:
                        href = link.get('href', '')
                        # ìºë¦­í„° í˜ì´ì§€ì¸ì§€ í™•ì¸ (/w/ë¡œ ì‹œì‘í•˜ê³  íŠ¹ì • íŒ¨í„´ ì œì™¸)
                        if (href.startswith('/w/') and 
                            'Character' not in href and 
                            'Personality' not in href and
                            'Special:' not in href and
                            'Category:' not in href):
                            
                            char_name = link.get_text(strip=True)
                            if char_name and len(char_name) > 1:
                                # ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ë¥¼ ê³ ë ¤í•œ ìºë¦­í„°ëª… ì •ë¦¬ (AS/ES ë“± ì œê±°)
                                # ë”•ì…”ë„ˆë¦¬ í‚¤ëŠ” ê¸°ë³¸ ì˜ì–´ëª…ìœ¼ë¡œ í†µì¼ (ë§¤ì¹­ ì‹œ ì¼ê´€ì„±ì„ ìœ„í•´)
                                base_char_name = char_name
                                style_patterns = [r'\s+(AS)$', r'\s+(ES)$', r'\s+(NS)$', 
                                                r'\s+(Another\s*Style)$', r'\s+(Extra\s*Style)$']
                                for pattern in style_patterns:
                                    match = re.search(pattern, char_name, re.IGNORECASE)
                                    if match:
                                        base_char_name = char_name[:match.start()]
                                        break
                                
                                # ìºë¦­í„°ì— í¼ìŠ¤ë„ë¦¬í‹° ì¶”ê°€ (ê¸°ë³¸ëª…ì„ í‚¤ë¡œ ì‚¬ìš©)
                                if base_char_name not in character_personalities:
                                    character_personalities[base_char_name] = []
                                if personality_kor not in character_personalities[base_char_name]:
                                    character_personalities[base_char_name].append(personality_kor)
        
        log_queue_ref.put(f"Found personality data for {len(character_personalities)} characters")
        
    except Exception as e:
        log_queue_ref.put(f"Error scraping personality page: {e}")
    
    return character_personalities

def scrape_personalities(detail_url, headers_ua, log_queue_ref):
    """
    ìºë¦­í„° ìƒì„¸ í˜ì´ì§€ì—ì„œ Personalities ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Returns:
        list: ì¶”ì¶œëœ í¼ìŠ¤ë„ë¦¬í‹° ë¦¬ìŠ¤íŠ¸
    """
    personality_mapping = load_personality_matching()
    personalities = []
    try:
        detail_resp = requests.get(detail_url, headers=headers_ua, timeout=10)
        detail_resp.raise_for_status()
        detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
        
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ í¼ìŠ¤ë„ë¦¬í‹° ì°¾ê¸°
        
        # ë°©ë²• 1: 'Personalities' idë¥¼ ê°€ì§„ span íƒœê·¸ ì°¾ê¸°
        headline = detail_soup.find('span', id='Personalities')
        if headline:
            parent_heading = headline.find_parent(['h2', 'h3'])
            if parent_heading:
                # ul íƒœê·¸ ì°¾ê¸°
                ul_tag = parent_heading.find_next_sibling('ul')
                if ul_tag:
                    for li in ul_tag.find_all('li'):
                        link = li.find('a')
                        if link:
                            personality_eng = link.get_text(strip=True)
                            personality_kor = personality_mapping.get(personality_eng, personality_eng)
                            if personality_kor and personality_kor not in personalities:
                                personalities.append(personality_kor)
        
        # ë°©ë²• 2: 'Personality' í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í—¤ë”© ì°¾ê¸°
        if not personalities:
            for heading in detail_soup.find_all(['h2', 'h3', 'h4']):
                if 'personalit' in heading.get_text().lower():
                    next_element = heading.find_next_sibling()
                    while next_element:
                        if next_element.name == 'ul':
                            for li in next_element.find_all('li'):
                                link = li.find('a')
                                if link:
                                    personality_eng = link.get_text(strip=True)
                                    personality_kor = personality_mapping.get(personality_eng, personality_eng)
                                    if personality_kor and personality_kor not in personalities:
                                        personalities.append(personality_kor)
                            break
                        elif next_element.name in ['h2', 'h3', 'h4']:
                            break
                        next_element = next_element.find_next_sibling()
        
        # ë°©ë²• 3: í˜ì´ì§€ì—ì„œ 'Personality:' í…ìŠ¤íŠ¸ ì§ì ‘ ì°¾ê¸°
        if not personalities:
            for element in detail_soup.find_all(text=re.compile(r'personalit', re.IGNORECASE)):
                parent = element.parent
                if parent:
                    # ì£¼ë³€ì—ì„œ ë§í¬ ì°¾ê¸°
                    nearby_links = parent.find_parent().find_all('a', href=True) if parent.find_parent() else []
                    for link in nearby_links:
                        href = link.get('href', '')
                        if '/w/Personalities/' in href or 'personality' in href.lower():
                            personality_eng = link.get_text(strip=True)
                            personality_kor = personality_mapping.get(personality_eng, personality_eng)
                            if personality_kor and personality_kor not in personalities:
                                personalities.append(personality_kor)
        
        if personalities:
            log_queue_ref.put(f"Found personalities: {', '.join(personalities)}")
        else:
            # ë””ë²„ê·¸ ì •ë³´: í˜ì´ì§€ì— ì‹¤ì œë¡œ Personalities ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
            personality_mentions = detail_soup.find_all(text=re.compile(r'personalit', re.IGNORECASE))
            if personality_mentions:
                log_queue_ref.put(f"Page mentions 'personality' {len(personality_mentions)} times but no structured data found")
            else:
                log_queue_ref.put("No personalities found on this page")
            
    except Exception as e:
        log_queue_ref.put(f"Error scraping personalities: {e}")
    
    return personalities

# --- ìºë¦­í„°ëª… ë§¤ì¹­ ë°ì´í„° ë¡œë“œ ---
def load_character_name_matching():
    """Matching_names.csvì—ì„œ ìºë¦­í„°ëª… ë§¤ì¹­ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    name_mapping = {}
    try:
        matching_csv_path = os.path.join(SCRIPT_DIR, "Matching_names.csv")
        if os.path.exists(matching_csv_path):
            df = pd.read_csv(matching_csv_path, encoding='utf-8-sig')
            for _, row in df.iterrows():
                if len(row) >= 2:
                    eng_name = row.iloc[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ (ì˜ì–´ëª…)
                    kor_name = row.iloc[1]  # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (í•œêµ­ì–´ëª…)
                    if pd.notna(eng_name) and pd.notna(kor_name):
                        name_mapping[eng_name] = kor_name
    except Exception as e:
        print(f"Error loading character name matching: {e}")
    return name_mapping

def convert_character_name_with_style(english_name, name_mapping):
    """
    ìºë¦­í„°ëª…ì„ í•œêµ­ì–´ë¡œ ë³€í™˜í•˜ë˜, AS/ES/NS ë“±ì˜ ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
    ì˜ˆ: "Shion ES" â†’ "ì‹œì˜¨ ES", "Aldo AS" â†’ "ì•Œë„ AS"
    
    Args:
        english_name (str): ì˜ì–´ ìºë¦­í„°ëª… (ìŠ¤íƒ€ì¼ í¬í•¨ ê°€ëŠ¥)
        name_mapping (dict): ì˜ì–´-í•œêµ­ì–´ ìºë¦­í„°ëª… ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        str: í•œêµ­ì–´ë¡œ ë³€í™˜ëœ ìºë¦­í„°ëª…
    """
    if not english_name:
        return english_name
    
    # ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ íŒ¨í„´ (AS, ES, NS, Another Style ë“±)
    style_patterns = [
        r'\s+(AS)$',           # " AS"
        r'\s+(ES)$',           # " ES" 
        r'\s+(NS)$',           # " NS"
        r'\s+(Another\s*Style)$',  # " Another Style"
        r'\s+(Extra\s*Style)$',    # " Extra Style"
        r'\s+(Manifestation)$',    # " Manifestation"
        r'\s+(Alter)$',            # " Alter"
    ]
    
    # ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ì°¾ê¸°
    base_name = english_name
    style_suffix = ""
    
    for pattern in style_patterns:
        match = re.search(pattern, english_name, re.IGNORECASE)
        if match:
            style_suffix = " " + match.group(1)
            base_name = english_name[:match.start()]
            break
    
    # ê¸°ë³¸ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜
    korean_base_name = name_mapping.get(base_name, base_name)
    
    # í•œêµ­ì–´ ê¸°ë³¸ ì´ë¦„ + ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬
    return korean_base_name + style_suffix

def scraping_logic_with_personalities(log_queue_ref, progress_queue_ref, selected_output_dir):
    """ê°œì„ ëœ ìŠ¤í¬ë ˆì´í•‘ ë¡œì§ - ì „ì²´ í¼ìŠ¤ë„ë¦¬í‹° í˜ì´ì§€ì™€ ê°œë³„ ìºë¦­í„° í˜ì´ì§€ ë³‘í–‰"""
    threading.current_thread().log_queue_ref = log_queue_ref
    threading.current_thread().progress_queue_ref = progress_queue_ref
    
    global IMAGE_DIR, STRUCTURE_ANALYSIS_FILENAME_CSV 
    IMAGE_DIR = os.path.join(selected_output_dir, "character_art")
    STRUCTURE_ANALYSIS_FILENAME_CSV = os.path.join(selected_output_dir, "structure_analysis.csv")
    
    setup_directories()
    log_queue_ref.put(f"Output directory set to: {selected_output_dir}")
    log_queue_ref.put("Loading character name and personality matching data...")
    
    # ìºë¦­í„°ëª… ë§¤ì¹­ ë°ì´í„° ë¡œë“œ
    name_mapping = load_character_name_matching()
    log_queue_ref.put(f"Loaded {len(name_mapping)} character name mappings")
    
    headers_ua = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    # 1. ì „ì²´ í¼ìŠ¤ë„ë¦¬í‹° í˜ì´ì§€ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    log_queue_ref.put("Step 1: Fetching personality data from comprehensive page...")
    all_personalities_data = scrape_all_personalities_from_page(headers_ua, log_queue_ref)
    
    # 2. ë©”ì¸ ìºë¦­í„° í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
    log_queue_ref.put(f"Step 2: Fetching main character data from: {TARGET_URL}")
    try:
        response = requests.get(TARGET_URL, headers=headers_ua, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        log_queue_ref.put(f"Error fetching page: {e}")
        progress_queue_ref.put({'done': True, 'error': True})
        return

    log_queue_ref.put("Page fetched. Parsing HTML...")
    soup = BeautifulSoup(response.content, 'html.parser')
    
    char_table = soup.find('table', class_='chara-table') or soup.find('table', class_='wikitable')
    if not char_table:
        log_queue_ref.put("Character table not found.")
        progress_queue_ref.put({'done': True, 'error': True})
        return

    log_queue_ref.put("Character table found. Parsing rows...")
    
    all_character_data_for_final_excel = []
    rows = char_table.find_all('tr')
    total_rows_to_process = len(rows) - 1
    
    log_queue_ref.put(f"Found {total_rows_to_process} potential character rows.")
    if total_rows_to_process <= 0:
        log_queue_ref.put("No character rows found.")
        progress_queue_ref.put({'done': True})
        return
        
    progress_queue_ref.put({'max': total_rows_to_process, 'value': 0})

    for i, row in enumerate(rows[1:]): 
        current_progress = i + 1
        progress_queue_ref.put({'value': current_progress}) 

        cells = row.find_all('td')
        if len(cells) < 4: 
            continue

        try:
            # 1. ë©”ì¸ í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ
            icon_cell = cells[0]
            icon_img_tag = icon_cell.find('img')
            icon_src = icon_img_tag['src'] if icon_img_tag else None
            icon_local_path = download_image(icon_src, "icons") if icon_src else None
            
            icon_filename = ""
            if icon_src:
                parsed = urlparse(icon_src)
                fname = parse_qs(parsed.query).get('f', [None])[0] or os.path.basename(unquote(parsed.path))
                icon_filename = fname.replace(' ', '_')

            name_rarity_cell = cells[1]
            name_tag = name_rarity_cell.find('a')
            name = name_tag.text.strip() if name_tag else ""
            
            # ìºë¦­í„°ëª… ë§¤ì¹­ (ì˜ì–´ -> í•œêµ­ì–´, ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ê³ ë ¤)
            korean_name = convert_character_name_with_style(name, name_mapping)
            
            rarity_text = name_rarity_cell.get_text(separator=" ").strip()
            rarity_match = re.search(r'\d(?:~\d)?â˜…(?:\s*\S+)?', rarity_text)
            rarity = rarity_match.group(0).strip() if rarity_match else ""

            element_equipment_cell = cells[2]
            ee_icon_tags = element_equipment_cell.find_all('img')
            element_equipment_icon_paths, element_equipment_icon_alts = [], []
            for img_tag in ee_icon_tags:
                ee_src, ee_alt = img_tag.get('src'), img_tag.get('alt', "")
                if ee_src:
                    local_path = download_image(ee_src, "elements_equipment")
                    if local_path:
                        element_equipment_icon_paths.append(local_path)
                        element_equipment_icon_alts.append(ee_alt)
            
            release_date = cells[3].text.strip()

            # 2. Personalities ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            personalities = []
            
            # ê¸°ë³¸ ìºë¦­í„°ëª… ì¶”ì¶œ (AS/ES ë“± ìŠ¤íƒ€ì¼ ì ‘ë¯¸ì‚¬ ì œê±°)
            base_name = name
            style_patterns = [r'\s+(AS)$', r'\s+(ES)$', r'\s+(NS)$', 
                            r'\s+(Another\s*Style)$', r'\s+(Extra\s*Style)$']
            for pattern in style_patterns:
                match = re.search(pattern, name, re.IGNORECASE)
                if match:
                    base_name = name[:match.start()]
                    break
            
            # ìš°ì„  ì „ì²´ í¼ìŠ¤ë„ë¦¬í‹° í˜ì´ì§€ì—ì„œ ì°¾ê¸° (ê¸°ë³¸ëª…ìœ¼ë¡œ ê²€ìƒ‰)
            if base_name in all_personalities_data:
                personalities = all_personalities_data[base_name]
                log_queue_ref.put(f"Found personalities from main page for '{name}' (base: '{base_name}'): {', '.join(personalities)}")
            else:
                # ê°œë³„ ìºë¦­í„° í˜ì´ì§€ì—ì„œ ìŠ¤í¬ë˜í•‘ (ë°±ì—…)
                detail_href = name_tag.get('href') if name_tag else None
                if detail_href:
                    detail_url = urljoin(BASE_URL, detail_href)
                    personalities = scrape_personalities(detail_url, headers_ua, log_queue_ref)
                    time.sleep(0.5)  # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸° ìœ„í•œ ì§€ì—°

            if name or icon_local_path:
                all_character_data_for_final_excel.append({
                    "icon_path": icon_local_path,
                    "icon_filename": icon_filename,
                    "name": name,
                    "korean_name": korean_name,  # í•œêµ­ì–´ ìºë¦­í„°ëª… ì¶”ê°€
                    "rarity": rarity,
                    "personalities": personalities,
                    "element_equipment_paths": element_equipment_icon_paths,
                    "element_equipment_alts": element_equipment_icon_alts,
                    "release_date": release_date
                })
                if current_progress % 10 == 0:
                    log_queue_ref.put(f"Row {current_progress}: Parsed & Downloaded for '{korean_name}' ({name}) - Personalities: {len(personalities)}")
        except Exception as e:
            log_queue_ref.put(f"Row {current_progress}: Error parsing: {e}")
            continue

    if not all_character_data_for_final_excel:
        log_queue_ref.put("No data to save to Excel.")
        progress_queue_ref.put({'done': True})
        return

    # --- ì—‘ì…€ íŒŒì¼ ìƒì„± (Personalities ì»¬ëŸ¼ í¬í•¨) ---
    final_excel_path_base = os.path.join(selected_output_dir, EXCEL_FILENAME_BASE)
    final_excel_full_path = get_unique_filename(f"{final_excel_path_base}.xlsx")
    log_queue_ref.put(f"Saving data to Excel: {final_excel_full_path}")
    
    wb = Workbook()
    ws = wb.active
    ws.title = "Characters"
    
    max_ee_icons = max([len(d["element_equipment_paths"]) for d in all_character_data_for_final_excel] or [0])

    # í—¤ë”ì— Korean Nameê³¼ Personalities ì¶”ê°€
    headers_excel = ["Icon", "Icon Filename", "Name", "Korean Name", "Rarity", "Personalities"]
    for i in range(max_ee_icons):
        headers_excel.extend([f"Elem/Equip {i+1} Icon", f"Elem/Equip {i+1} Alt"])
    headers_excel.append("Release Date")
    ws.append(headers_excel)

    # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
    ws.column_dimensions[get_column_letter(1)].width = 12  # Icon
    ws.column_dimensions[get_column_letter(2)].width = 35  # Icon Filename
    ws.column_dimensions[get_column_letter(3)].width = 30  # Name
    ws.column_dimensions[get_column_letter(4)].width = 30  # Korean Name
    ws.column_dimensions[get_column_letter(5)].width = 15  # Rarity
    ws.column_dimensions[get_column_letter(6)].width = 50  # Personalities (ë„“ê²Œ)
    
    ee_start_col = 6
    for i in range(max_ee_icons):
        ws.column_dimensions[get_column_letter(ee_start_col + i*2)].width = 12 
        ws.column_dimensions[get_column_letter(ee_start_col + i*2 + 1)].width = 25 
    ws.column_dimensions[get_column_letter(ee_start_col + max_ee_icons*2)].width = 15

    for row_idx_excel, char_data in enumerate(all_character_data_for_final_excel, start=2):
        ws.row_dimensions[row_idx_excel].height = 60 
        
        # Icon
        current_col = 1
        if char_data["icon_path"] and os.path.exists(char_data["icon_path"]):
            try:
                img = OpenpyxlImage(char_data["icon_path"])
                img.height, img.width = 75, 75
                ws.add_image(img, f'{get_column_letter(current_col)}{row_idx_excel}')
            except Exception: 
                pass
        current_col += 1
        
        # Icon Filename, Name, Korean Name, Rarity, Personalities
        ws.cell(row=row_idx_excel, column=current_col, value=char_data.get("icon_filename", ""))
        current_col += 1
        ws.cell(row=row_idx_excel, column=current_col, value=char_data.get("name", ""))
        current_col += 1
        ws.cell(row=row_idx_excel, column=current_col, value=char_data.get("korean_name", ""))
        current_col += 1
        ws.cell(row=row_idx_excel, column=current_col, value=char_data.get("rarity", ""))
        current_col += 1
        
        # Personalities ë°ì´í„° ì¶”ê°€
        personalities_text = ", ".join(char_data.get("personalities", []))
        ws.cell(row=row_idx_excel, column=current_col, value=personalities_text)
        current_col += 1
        
        # Elem/Equip
        for i in range(max_ee_icons):
            if i < len(char_data["element_equipment_paths"]):
                icon_path = char_data["element_equipment_paths"][i]
                icon_alt = char_data["element_equipment_alts"][i]
                if icon_path and os.path.exists(icon_path):
                    try:
                        img = OpenpyxlImage(icon_path)
                        img.height, img.width = 30, 30
                        ws.add_image(img, f'{get_column_letter(current_col)}{row_idx_excel}')
                    except Exception: 
                        pass
                ws.cell(row=row_idx_excel, column=current_col + 1, value=icon_alt)
            current_col += 2
        
        # Release Date
        ws.cell(row=row_idx_excel, column=current_col, value=char_data.get("release_date", ""))
        
    try:
        wb.save(final_excel_full_path)
        log_queue_ref.put(f"Excel file saved successfully: {final_excel_full_path}")
        
        # CSV ìƒì„±ë„ í•¨ê»˜ ìˆ˜í–‰
        generate_csv_with_personalities(final_excel_full_path, selected_output_dir, log_queue_ref)
        
    except Exception as e:
        log_queue_ref.put(f"Error saving Excel file: {e}")
        progress_queue_ref.put({'done': True, 'error': True, 'error_message': f"Excel ì €ì¥ ì˜¤ë¥˜:\n{e}"})
        return
    
    progress_queue_ref.put({'done': True})

def generate_csv_with_personalities(excel_path, output_dir, log_queue_ref):
    """ì—‘ì…€ì—ì„œ CSV ìƒì„± (Personalities í¬í•¨)"""
    try:
        df = pd.read_excel(excel_path, engine='openpyxl')
        
        # ì´ë¦„ ë§¤ì¹­ íŒŒì¼ ë¡œë“œ
        name_map = {}
        try:
            name_map_df = pd.read_csv("Matching_names.csv")
            name_map = dict(zip(name_map_df["ìºë¦­í„°ëª… (ì…ë ¥)"], name_map_df["ìºë¦­í„°ëª… (ë§¤ì¹­)"]))
        except FileNotFoundError:
            log_queue_ref.put("Warning: Matching_names.csv not found. Using English names.")
        
        # CSV ë°ì´í„° ìƒì„±
        result = []
        for _, row in df.iterrows():
            name = str(row.get('Name', '')).strip()
            if not name:
                continue
                
            # í•œê¸€ ì´ë¦„ ë³€í™˜
            korean_name = name_map.get(name, name)
            
            result.append({
                "ìºë¦­í„°ëª…": korean_name,
                "í¬ê·€ë„": str(row.get('Rarity', '')).strip(),
                "ìºë¦­í„°ì•„ì´ì½˜ê²½ë¡œ": "",  # ì´ë¯¸ì§€ ê²½ë¡œëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
                "ê°œì„±(í¼ìŠ¤ë„ë¦¬í‹°)": str(row.get('Personalities', '')).strip(),  # ìƒˆë¡œ ì¶”ê°€
                "ì†ì„±ëª…ë¦¬ìŠ¤íŠ¸": "",  # ì†ì„± ë§¤í•‘ í•„ìš”
                "ì†ì„±_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸": "",
                "ë¬´ê¸°ëª…ë¦¬ìŠ¤íŠ¸": "",  # ë¬´ê¸° ë§¤í•‘ í•„ìš”
                "ë¬´ê¸°_ì•„ì´ì½˜ê²½ë¡œë¦¬ìŠ¤íŠ¸": "",
                "ì¶œì‹œì¼": str(row.get('Release Date', '')).strip(),
            })
        
        # CSV ì €ì¥
        csv_path = os.path.join(output_dir, "eden_roulette_data_with_personalities.csv")
        pd.DataFrame(result).to_csv(csv_path, index=False, encoding="utf-8-sig")
        log_queue_ref.put(f"CSV file generated: {csv_path}")
        
    except Exception as e:
        log_queue_ref.put(f"Error generating CSV: {e}")

# GUI í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ Personalities í¬í•¨)
class PersonalityScraperApp:
    def __init__(self, root_window):
        self.root = root_window
        root_window.title("ì–´ë‚˜ë”ì—ë´ ìºë¦­í„° ìŠ¤í¬ë ˆì´í¼ (Personalities í¬í•¨)")
        root_window.geometry("900x700")

        load_config()
        self.output_dir_var = tk.StringVar(value=OUTPUT_BASE_DIR)

        style = ttk.Style()
        try:
            if os.name == 'nt': 
                style.theme_use('vista')
            else: 
                style.theme_use('clam') 
        except tk.TclError: 
            style.theme_use('clam')

        main_frame = ttk.Frame(root_window, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)

        # ê²½ë¡œ ì„¤ì •
        path_frame = ttk.LabelFrame(main_frame, text="ì¶œë ¥ ì„¤ì •", padding="10")
        path_frame.pack(fill=tk.X, pady=(0,10))
        
        ttk.Label(path_frame, text="ì €ì¥ í´ë”:").grid(row=0, column=0, padx=(0,5), pady=5, sticky="w")
        self.output_dir_entry = ttk.Entry(path_frame, textvariable=self.output_dir_var, width=60)
        self.output_dir_entry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")
        self.btn_browse_dir = ttk.Button(path_frame, text="í´ë” ì°¾ì•„ë³´ê¸°", command=self.browse_output_directory)
        self.btn_browse_dir.grid(row=0, column=2, padx=(5,0), pady=5)
        path_frame.columnconfigure(1, weight=1)

        # ì»¨íŠ¸ë¡¤
        control_frame = ttk.Frame(main_frame)
        control_frame.pack(fill=tk.X, pady=(0,10))

        self.btn_scrape = ttk.Button(
            control_frame, 
            text="ğŸ” ìºë¦­í„° ë°ì´í„° ìˆ˜ì§‘ (Personalities í¬í•¨)", 
            command=self.start_scraping_thread
        )
        self.btn_scrape.pack(side=tk.LEFT, padx=5, pady=5, fill=tk.X, expand=True)
        
        self.btn_open_folder = ttk.Button(control_frame, text="ğŸ“ ì¶œë ¥ í´ë” ì—´ê¸°", command=self.open_output_folder)
        self.btn_open_folder.pack(side=tk.LEFT, padx=5, pady=5, fill=tk.X, expand=True)

        # ë¡œê·¸
        log_label_frame = ttk.LabelFrame(main_frame, text="ì§„í–‰ ìƒí™© ë° ë¡œê·¸", padding="10")
        log_label_frame.pack(fill=tk.BOTH, expand=True)

        self.log_text = scrolledtext.ScrolledText(
            log_label_frame, wrap=tk.WORD, state=tk.DISABLED, height=20, 
            font=("Malgun Gothic", 9) if os.name == 'nt' else ("TkDefaultFont", 10)
        )
        self.log_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        self.log_text.tag_configure("error", foreground="red")
        self.log_text.tag_configure("info", foreground="blue")
        self.log_text.tag_configure("success", foreground="green")

        self.progress_bar = ttk.Progressbar(log_label_frame, orient="horizontal", length=300, mode="determinate")
        self.progress_bar.pack(pady=10, fill=tk.X, padx=5)

        self.log_queue = queue.Queue()
        self.progress_queue = queue.Queue()
        self.root.after(100, self.process_queues)
        
        root_window.protocol("WM_DELETE_WINDOW", self.on_closing)

    def on_closing(self):
        save_config()
        self.root.destroy()

    def browse_output_directory(self):
        global OUTPUT_BASE_DIR
        selected_dir = filedialog.askdirectory(initialdir=self.output_dir_var.get())
        if selected_dir:
            self.output_dir_var.set(selected_dir)
            OUTPUT_BASE_DIR = selected_dir
            self.log_message(f"ì¶œë ¥ í´ë”ê°€ ë‹¤ìŒìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {selected_dir}", "info")
            save_config()

    def open_output_folder(self):
        folder_to_open = self.output_dir_var.get()
        if not os.path.isdir(folder_to_open):
            messagebox.showerror("ì˜¤ë¥˜", f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{folder_to_open}")
            return
        try:
            if os.name == 'nt': 
                os.startfile(folder_to_open)
            elif sys.platform == "darwin": 
                os.system(f'open "{folder_to_open}"')
            else: 
                os.system(f'xdg-open "{folder_to_open}"')
        except Exception as e:
            messagebox.showerror("í´ë” ì—´ê¸° ì˜¤ë¥˜", f"í´ë”ë¥¼ ì—¬ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{e}")

    def log_message(self, message, tag=None):
        self.log_text.config(state=tk.NORMAL)
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        self.log_text.insert(tk.END, f"[{timestamp}] {message}\n", tag)
        self.log_text.config(state=tk.DISABLED)
        self.log_text.see(tk.END) 

    def start_scraping_thread(self):
        current_output_dir = self.output_dir_var.get()
        if not os.path.isdir(current_output_dir):
            messagebox.showerror("ì˜¤ë¥˜", f"ìœ íš¨í•œ ì¶œë ¥ í´ë”ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        self.btn_scrape.config(state=tk.DISABLED)
        self.progress_bar["value"] = 0
        self.log_text.config(state=tk.NORMAL)
        self.log_text.delete('1.0', tk.END)
        self.log_text.config(state=tk.DISABLED)
        self.log_message("Personalities í¬í•¨ ìŠ¤í¬ë ˆì´í•‘ ì‹œì‘...", "info")

        save_config()

        self.thread = threading.Thread(
            target=scraping_logic_with_personalities, 
            args=(self.log_queue, self.progress_queue, current_output_dir), 
            daemon=True
        )
        self.thread.start()

    def process_queues(self):
        try:
            while True: 
                message = self.log_queue.get_nowait()
                tag = "error" if "Error" in message or "ì˜¤ë¥˜" in message else "success" if "saved" in message or "ì™„ë£Œ" in message else "info"
                self.log_message(message, tag)
        except queue.Empty: 
            pass

        try:
            while True: 
                progress_data = self.progress_queue.get_nowait()
                if 'max' in progress_data: 
                    self.progress_bar["maximum"] = progress_data['max']
                if 'value' in progress_data: 
                    self.progress_bar["value"] = progress_data['value']
                if progress_data.get('done'):
                    self.btn_scrape.config(state=tk.NORMAL)
                    if self.progress_bar["value"] < self.progress_bar["maximum"]:
                        self.progress_bar["value"] = self.progress_bar["maximum"] 
                    
                    if progress_data.get('error'):
                        msg = progress_data.get('error_message', "ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ.")
                        messagebox.showerror("ì˜¤ë¥˜", msg)
                    else:
                        messagebox.showinfo("ì™„ë£Œ", "Personalities í¬í•¨ ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        except queue.Empty: 
            pass
        
        self.root.after(100, self.process_queues)

if __name__ == "__main__":
    root = tk.Tk()
    app = PersonalityScraperApp(root)
    root.mainloop()